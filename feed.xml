<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hugging Face Papers RSS</title>
    <link>https://huggingface.co/papers</link>
    <description>Latest AI research papers from Hugging Face</description>
    <lastBuildDate>Mon, 02 Mar 2026 10:30:19 GMT</lastBuildDate>
    <atom:link href="https://AzureSilent.github.io/hf-paper-rss/feed.xml" rel="self" type="application/rss+xml"/>
    <item>
      <title>DLEBench: Evaluating Small-scale Object Editing Ability for Instruction-based Image Editing Model</title>
      <link>https://huggingface.co/papers/2602.23622</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Shibo Hong, Boxian Ai, Jun Kuang, Wei Wang, FengJiao Chen</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.23622">arXiv</a> | <a href="https://arxiv.org/pdf/2602.23622.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract A new benchmark called DeepLookEditBench is introduced to evaluate instruction-based image editing models' capability in handling small-scale object editing, revealing significant performance gaps in this area. 
					AI-generated summary</p>
  <p>摘要 引入了一种名为 DeepLo​​okEditBench 的新基准来评估基于指令的图像编辑模型处理小规模对象编辑的能力，揭示了该领域的显着性能差距。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>Significant progress has been made in the field of Instruction-based Image Editing Models (IIEMs). However, while these models demonstrate plausible adherence to instructions and strong reasoning ability on current benchmarks, their ability to edit small objects remains underexplored, despite its importance for precise local editing and refining details in both real and generated images. In this paper, we introduce DeepLookEditBench (DLEBench), the first benchmark dedicated to assessing the abilities of IIEMs in editing small-scale objects. Specifically, we construct a challenging testbed comprising 1889 samples across seven instruction types. In these samples, target objects occupy only 1%-10% of the image area, covering complex scenarios such as partial occlusion and multi-object editing. To ensure robust evaluation on this benchmark, we propose an evaluation protocol with refined score rubrics to minimize subjectivity and ambiguity in two criteria: Instruction Following and Visual Consistency. This protocol also introduces a dual-mode evaluation framework (Tool-driven and Oracle-guided Modes) addressing the misalignment between LMM-as-a-Judge and human judgements on DLEBench. Empirical results on 10 IIEMs reveal significant performance gaps in small-scale object editing, highlighting the need for specialized benchmarks to advance this ability.</p>
  <p>基于指令的图像编辑模型（IIEM）领域已经取得了重大进展。然而，虽然这些模型在当前基准上表现出对指令的合理遵守和强大的推理能力，但它们编辑小对象的能力仍未得到充分探索，尽管它对于精确的本地编辑和细化真实图像和生成图像中的细节很重要。在本文中，我们介绍了 DeepLo​​okEditBench (DLEBench)，这是第一个致力于评估 IIEM 编辑小规模对象能力的基准测试。具体来说，我们构建了一个具有挑战性的测试床，其中包含 7 种指令类型的 1889 个样本。在这些样本中，目标对象仅占据图像区域的1%-10%，涵盖了部分遮挡、多对象编辑等复杂场景。为了确保对该基准的稳健评估，我们提出了一个具有精细评分标准的评估协议，以最大限度地减少两个标准的主观性和模糊性：指令遵循和视觉一致性。该协议还引入了双模式评估框架（工具驱动和 Oracle 引导模式），解决了 LMM 作为法官与 DLEBench 上人类判断之间的不一致问题。 10 个 IIEM 的实证结果揭示了小规模对象编辑中的显着性能差距，凸显了需要专门的基准来提高这种能力。</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 03:59:31 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.23622</guid>
    </item>
    <item>
      <title>Vectorizing the Trie: Efficient Constrained Decoding for LLM-based Generative Retrieval on Accelerators</title>
      <link>https://huggingface.co/papers/2602.22647</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Zhengyang Su, Isay Katsman, Yueqi Wang, Ruining He, Lukasz Heldt</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.22647">arXiv</a> | <a href="https://arxiv.org/pdf/2602.22647.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract STATIC is an efficient constrained decoding method that uses a flattened prefix tree represented as a sparse matrix to accelerate generative retrieval on hardware accelerators while maintaining low latency and high throughput. 
					AI-generated summary</p>
  <p>摘要 STATIC 是一种高效的约束解码方法，它使用表示为稀疏矩阵的扁平化前缀树来加速硬件加速器上的生成检索，同时保持低延迟和高吞吐量。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>Generative retrieval has emerged as a powerful paradigm for LLM-based recommendation. However, industrial recommender systems often benefit from restricting the output space to a constrained subset of items based on business logic (e.g. enforcing content freshness or product category), which standard autoregressive decoding cannot natively support. Moreover, existing constrained decoding methods that make use of prefix trees (Tries) incur severe latency penalties on hardware accelerators (TPUs/GPUs). In this work, we introduce STATIC (Sparse Transition Matrix-Accelerated Trie Index for Constrained Decoding), an efficient and scalable constrained decoding technique designed specifically for high-throughput LLM-based generative retrieval on TPUs/GPUs. By flattening the prefix tree into a static Compressed Sparse Row (CSR) matrix, we transform irregular tree traversals into fully vectorized sparse matrix operations, unlocking massive efficiency gains on hardware accelerators. We deploy STATIC on a large-scale industrial video recommendation platform serving billions of users. STATIC produces significant product metric impact with minimal latency overhead (0.033 ms per step and 0.25% of inference time), achieving a 948x speedup over a CPU trie implementation and a 47-1033x speedup over a hardware-accelerated binary-search baseline. Furthermore, the runtime overhead of STATIC remains extremely low across a wide range of practical configurations. To the best of our knowledge, STATIC enables the first production-scale deployment of strictly constrained generative retrieval. In addition, evaluation on academic benchmarks demonstrates that STATIC can considerably improve cold-start performance for generative retrieval. Our code is available at https://github.com/youtube/static-constraint-decoding.</p>
  <p>生成检索已成为基于 LLM 的推荐的强大范例。然而，工业推荐系统通常受益于基于业务逻辑将输出空间限制为受约束的项目子集（例如强制内容新鲜度或产品类别），而标准自回归解码本身无法支持这一点。此外，现有的使用前缀树（Tries）的受限解码方法会对硬件加速器（TPU/GPU）造成严重的延迟损失。在这项工作中，我们引入了 STATIC（用于约束解码的稀疏转换矩阵加速 Trie 索引），这是一种高效且可扩展的约束解码技术，专为 TPU/GPU 上基于 LLM 的高吞吐量生成检索而设计。通过将前缀树展平为静态压缩稀疏行（CSR）矩阵，我们将不规则的树遍历转换为完全矢量化的稀疏矩阵运算，从而在硬件加速器上释放巨大的效率提升。我们将STATIC部署在服务数十亿用户的大型工业视频推荐平台上。 STATIC 以最小的延迟开销（每步 0.033 毫秒和 0.25% 的推理时间）产生显着的产品指标影响，与 CPU trie 实现相比实现了 948 倍的加速，比硬件加速的二进制搜索基线实现了 47-1033 倍的加速。此外，在各种实际配置中，STATIC 的运行时开销仍然极低。据我们所知，STATIC 首次实现了严格约束生成检索的生产规模部署。此外，对学术基准的评估表明，STATIC 可以显着提高生成检索的冷启动性能。我们的代码位于 https://github.com/youtube/static-constraint-decoding。</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 03:59:32 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.22647</guid>
    </item>
    <item>
      <title>LongVideo-R1: Smart Navigation for Low-cost Long Video Understanding</title>
      <link>https://huggingface.co/papers/2602.20913</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: ucas | Authors: Jihao Qiu, Lingxi Xie, Xinyue Huo, Qi Tian, Qixiang Ye</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.20913">arXiv</a> | <a href="https://arxiv.org/pdf/2602.20913.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract A multimodal large language model agent called LongVideo-R1 is presented for efficient long video understanding with reduced computational requirements through active reasoning and selective clip navigation. 
					AI-generated summary</p>
  <p>摘要：提出了一种名为 LongVideo-R1 的多模态大语言模型代理，用于通过主动推理和选择性剪辑导航减少计算需求，从而实现高效的长视频理解。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>This paper addresses the critical and underexplored challenge of long video understanding with low computational budgets. We propose LongVideo-R1, an active, reasoning-equipped multimodal large language model (MLLM) agent designed for efficient video context navigation, avoiding the redundancy of exhaustive search. At the core of LongVideo-R1 lies a reasoning module that leverages high-level visual cues to infer the most informative video clip for subsequent processing. During inference, the agent initiates traversal from top-level visual summaries and iteratively refines its focus, immediately halting the exploration process upon acquiring sufficient knowledge to answer the query. To facilitate training, we first extract hierarchical video captions from CGBench, a video corpus with grounding annotations, and guide GPT-5 to generate 33K high-quality chain-of-thought-with-tool trajectories. The LongVideo-R1 agent is fine-tuned upon the Qwen-3-8B model through a two-stage paradigm: supervised fine-tuning (SFT) followed by reinforcement learning (RL), where RL employs a specifically designed reward function to maximize selective and efficient clip navigation. Experiments on multiple long video benchmarks validate the effectiveness of name, which enjoys superior tradeoff between QA accuracy and efficiency. All curated data and source code are provided in the supplementary material and will be made publicly available. Code and data are available at: https://github.com/qiujihao19/LongVideo-R1</p>
  <p>本文解决了低计算预算下长视频理解的关键且尚未充分探索的挑战。我们提出了 LongVideo-R1，一种主动的、配备推理的多模态大语言模型（MLLM）代理，设计用于高效的视频上下文导航，避免详尽搜索的冗余。 LongVideo-R1 的核心是一个推理模块，它利用高级视觉线索来推断信息最丰富的视频剪辑，以供后续处理。在推理过程中，代理从顶级视觉摘要开始遍历，并迭代地细化其焦点，在获得足够的知识来回答查询后立即停止探索过程。为了便于训练，我们首先从带有基础注释的视频语料库 CGBench 中提取分层视频字幕，并引导 GPT-5 生成 33K 高质量的思想链工具轨迹。 LongVideo-R1 代理通过两阶段范式在 Qwen-3-8B 模型上进行微调：监督微调 (SFT)，然后是强化学习 (RL)，其中 RL 采用专门设计的奖励函数来最大限度地提高选择性和高效的剪辑导航。在多个长视频基准上进行的实验验证了 name 的有效性，它在 QA 准确性和效率之间享有卓越的权衡。所有精选数据和源代码均在补充材料中提供，并将公开发布。代码和数据可参见：https://github.com/qiujihao19/LongVideo-R1</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 08:27:28 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.20913</guid>
    </item>
    <item>
      <title>SenCache: Accelerating Diffusion Model Inference via Sensitivity-Aware Caching</title>
      <link>https://huggingface.co/papers/2602.24208</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Yasaman Haghighi, Alexandre Alahi</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.24208">arXiv</a> | <a href="https://arxiv.org/pdf/2602.24208.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract A sensitivity-aware caching framework improves diffusion model inference efficiency by dynamically selecting cache timesteps based on model output sensitivity to input perturbations. 
					AI-generated summary</p>
  <p>摘要：敏感度感知缓存框架通过根据模型输出对输入扰动的敏感度动态选择缓存时间步长，从而提高扩散模型推理效率。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>Diffusion models achieve state-of-the-art video generation quality, but their inference remains expensive due to the large number of sequential denoising steps. This has motivated a growing line of research on accelerating diffusion inference. Among training-free acceleration methods, caching reduces computation by reusing previously computed model outputs across timesteps. Existing caching methods rely on heuristic criteria to choose cache/reuse timesteps and require extensive tuning. We address this limitation with a principled sensitivity-aware caching framework. Specifically, we formalize the caching error through an analysis of the model output sensitivity to perturbations in the denoising inputs, i.e., the noisy latent and the timestep, and show that this sensitivity is a key predictor of caching error. Based on this analysis, we propose Sensitivity-Aware Caching (SenCache), a dynamic caching policy that adaptively selects caching timesteps on a per-sample basis. Our framework provides a theoretical basis for adaptive caching, explains why prior empirical heuristics can be partially effective, and extends them to a dynamic, sample-specific approach. Experiments on Wan 2.1, CogVideoX, and LTX-Video show that SenCache achieves better visual quality than existing caching methods under similar computational budgets.</p>
  <p>扩散模型实现了最先进的视频生成质量，但由于大量的连续去噪步骤，其推理仍然昂贵。这推动了越来越多关于加速扩散推理的研究。在免训练加速方法中，缓存通过跨时间步重用先前计算的模型输出来减少计算量。现有的缓存方法依赖于启发式标准来选择缓存/重用时间步长，并且需要大量的调整。我们通过原则性的敏感度感知缓存框架来解决这一限制。具体来说，我们通过分析模型输出对去噪输入中的扰动（即噪声潜伏和时间步长）的敏感性来形式化缓存误差，并表明这种敏感性是缓存误差的关键预测因子。基于此分析，我们提出了灵敏度感知缓存（SenCache），这是一种动态缓存策略，可以根据每个样本自适应地选择缓存时间步长。我们的框架为自适应缓存提供了理论基础，解释了为什么先前的经验启发法可以部分有效，并将其扩展到动态的、特定于样本的方法。在 Wan 2.1、CogVideoX 和 LTX-Video 上的实验表明，在类似的计算预算下，SenCache 比现有的缓存方法实现了更好的视觉质量。</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 03:59:30 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.24208</guid>
    </item>
    <item>
      <title>Ref-Adv: Exploring MLLM Visual Reasoning in Referring Expression Tasks</title>
      <link>https://huggingface.co/papers/2602.23898</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Northeastern University | Authors: Qihua Dong, Kuo Yang, Lin Ju, Handong Zhao, Yitian Zhang</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.23898">arXiv</a> | <a href="https://arxiv.org/pdf/2602.23898.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract Ref-Adv is a challenging benchmark for referring expression comprehension that eliminates shortcut solutions by using complex linguistic expressions with minimal identifying information and hard distractors, revealing limitations in current multimodal LLMs' visual reasoning capabilities. 
					AI-generated summary</p>
  <p>摘要 Ref-Adv 是指称表达理解的一个具有挑战性的基准，它通过使用复杂的语言表达、最少的识别信息和硬干扰来消除捷径解决方案，揭示了当前多模态法学硕士视觉推理能力的局限性。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>Referring Expression Comprehension (REC) links language to region level visual perception. Standard benchmarks (RefCOCO, RefCOCO+, RefCOCOg) have progressed rapidly with multimodal LLMs but remain weak tests of visual reasoning and grounding: (i) many expressions are very short, leaving little reasoning demand; (ii) images often contain few distractors, making the target easy to find; and (iii) redundant descriptors enable shortcut solutions that bypass genuine text understanding and visual reasoning. We introduce Ref-Adv, a modern REC benchmark that suppresses shortcuts by pairing linguistically nontrivial expressions with only the information necessary to uniquely identify the target. The dataset contains referring expressions on real images, curated with hard distractors and annotated with reasoning facets including negation. We conduct comprehensive ablations (word order perturbations and descriptor deletion sufficiency) to show that solving Ref-Adv requires reasoning beyond simple cues, and we evaluate a broad suite of contemporary multimodal LLMs on Ref-Adv. Despite strong results on RefCOCO, RefCOCO+, and RefCOCOg, models drop markedly on Ref-Adv, revealing reliance on shortcuts and gaps in visual reasoning and grounding. We provide an in depth failure analysis and aim for Ref-Adv to guide future work on visual reasoning and grounding in MLLMs.</p>
  <p>引用表达理解 (REC) 将语言与区域级视觉感知联系起来。标准基准（RefCOCO、RefCOCO+、RefCOCOg）在多模式法学硕士方面取得了快速进展，但对视觉推理和基础的测试仍然薄弱：（i）许多表达式非常短，几乎没有推理需求； (ii) 图像通常包含很少的干扰因素，使目标易于找到； (iii) 冗余描述符可以实现绕过真正的文本理解和视觉推理的快捷解决方案。我们引入了 Ref-Adv，这是一种现代 REC 基准，它通过将语言上不平凡的表达式与唯一识别目标所需的信息配对来抑制捷径。该数据集包含真实图像上的引用表达式，用硬干扰项进行整理，并用包括否定在内的推理方面进行注释。我们进行了全面的消融（词序扰动和描述符删除充分性），以表明解决 Ref-Adv 需要超越简单线索的推理，并且我们在 Ref-Adv 上评估了一系列广泛的当代多模式法学硕士。尽管 RefCOCO、RefCOCO+ 和 RefCOCOg 上的结果很好，但模型在 Ref-Adv 上的表现明显下降，这揭示了对捷径的依赖以及视觉推理和基础方面的差距。我们提供深入的故障分析，并希望 Ref-Adv 能够指导 MLLM 中视觉推理和基础的未来工作。</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 03:59:29 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.23898</guid>
    </item>
    <item>
      <title>InfoNCE Induces Gaussian Distribution</title>
      <link>https://huggingface.co/papers/2602.24012</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Technion Israel institute of technology | Authors: Roy Betser, Eyal Gofer, Meir Yossef Levi, Guy Gilboa</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.24012">arXiv</a> | <a href="https://arxiv.org/pdf/2602.24012.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract Contrastive learning with InfoNCE objective creates Gaussian-like structures in learned representations, supported by theoretical analysis and experimental validation across different datasets and architectures. 
					AI-generated summary</p>
  <p>摘要：基于 InfoNCE 目标的对比学习在学习表示中创建类高斯结构，并得到不同数据集和架构的理论分析和实验验证的支持。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>Contrastive learning has become a cornerstone of modern representation learning, allowing training with massive unlabeled data for both task-specific and general (foundation) models. A prototypical loss in contrastive training is InfoNCE and its variants. In this work, we show that the InfoNCE objective induces Gaussian structure in representations that emerge from contrastive training. We establish this result in two complementary regimes. First, we show that under certain alignment and concentration assumptions, projections of the high-dimensional representation asymptotically approach a multivariate Gaussian distribution. Next, under less strict assumptions, we show that adding a small asymptotically vanishing regularization term that promotes low feature norm and high feature entropy leads to similar asymptotic results. We support our analysis with experiments on synthetic and CIFAR-10 datasets across multiple encoder architectures and sizes, demonstrating consistent Gaussian behavior. This perspective provides a principled explanation for commonly observed Gaussianity in contrastive representations. The resulting Gaussian model enables principled analytical treatment of learned representations and is expected to support a wide range of applications in contrastive learning.</p>
  <p>对比学习已成为现代表示学习的基石，允许使用大量未标记数据对特定任务模型和通用（基础）模型进行训练。对比训练中的典型损失是 InfoNCE 及其变体。在这项工作中，我们展示了 InfoNCE 目标在对比训练中出现的表示中引入高斯结构。我们在两个互补的机制中建立了这个结果。首先，我们表明，在某些对齐和集中假设下，高维表示的投影渐近地接近多元高斯分布。接下来，在不太严格的假设下，我们表明添加一个小的渐近消失正则化项来促进低特征范数和高特征熵会导致类似的渐近结果。我们通过跨多个编码器架构和大小的合成数据集和 CIFAR-10 数据集的实验来支持我们的分析，展示了一致的高斯行为。这种观点为对比表示中常见的高斯性提供了原则性的解释。由此产生的高斯模型能够对学习到的表示进行有原则的分析处理，并有望支持对比学习中的广泛应用。</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 08:27:25 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.24012</guid>
    </item>
    <item>
      <title>Compositional Generalization Requires Linear, Orthogonal Representations in Vision Embedding Models</title>
      <link>https://huggingface.co/papers/2602.24264</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Arnas Uselis, Andrea Dittadi, Seong Joon Oh</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.24264">arXiv</a> | <a href="https://arxiv.org/pdf/2602.24264.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract Compositional generalization requires neural representations to decompose linearly into orthogonal per-concept components, with empirical evidence showing this structure emerges in vision models like CLIP and DINO. 
					AI-generated summary</p>
  <p>摘要 组合泛化需要神经表征线性分解为正交的每个概念组件，经验证据表明这种结构出现在 CLIP 和 DINO 等视觉模型中。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>Compositional generalization, the ability to recognize familiar parts in novel contexts, is a defining property of intelligent systems. Although modern models are trained on massive datasets, they still cover only a tiny fraction of the combinatorial space of possible inputs, raising the question of what structure representations must have to support generalization to unseen combinations. We formalize three desiderata for compositional generalization under standard training (divisibility, transferability, stability) and show they impose necessary geometric constraints: representations must decompose linearly into per-concept components, and these components must be orthogonal across concepts. This provides theoretical grounding for the Linear Representation Hypothesis: the linear structure widely observed in neural representations is a necessary consequence of compositional generalization. We further derive dimension bounds linking the number of composable concepts to the embedding geometry. Empirically, we evaluate these predictions across modern vision models (CLIP, SigLIP, DINO) and find that representations exhibit partial linear factorization with low-rank, near-orthogonal per-concept factors, and that the degree of this structure correlates with compositional generalization on unseen combinations. As models continue to scale, these conditions predict the representational geometry they may converge to. Code is available at https://github.com/oshapio/necessary-compositionality.</p>
  <p>组合概括，即在新环境中识别熟悉部分的能力，是智能系统的一个定义属性。尽管现代模型是在海量数据集上进行训练的，但它们仍然只覆盖了可能输入的组合空间的一小部分，这就提出了一个问题：什么样的结构表示必须支持对未见过的组合的泛化。我们在标准训练下形式化了组合泛化的三个需求（可分性、可转移性、稳定性），并表明它们施加了必要的几何约束：表示必须线性分解为每个概念的组件，并且这些组件必须在概念之间正交。这为线性表示假说提供了理论基础：在神经表示中广泛观察到的线性结构是组合泛化的必然结果。我们进一步推导出将可组合概念的数量与嵌入几何图形联系起来的维度界限。根据经验，我们评估了现代视觉模型（CLIP、SigLIP、DINO）的这些预测，并发现表示表现出具有低秩、接近正交的每个概念因子的部分线性分解，并且这种结构的程度与未见组合的组合概括相关。随着模型继续扩展，这些条件预测它们可能收敛到的表征几何。代码可在 https://github.com/oshapio/necessary-compositionality 获取。</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 10:30:25 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.24264</guid>
    </item>
    <item>
      <title>LK Losses: Direct Acceptance Rate Optimization for Speculative Decoding</title>
      <link>https://huggingface.co/papers/2602.23881</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Nebius | Authors: Alexander Samarin, Sergei Krutikov, Anton Shevtsov, Sergei Skvortsov, Filipp Fisin</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.23881">arXiv</a> | <a href="https://arxiv.org/pdf/2602.23881.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract Speculative decoding speedup is improved by LK losses that directly optimize acceptance rate instead of KL divergence, showing consistent performance gains across different model sizes and domains. 
					AI-generated summary</p>
  <p>摘要 通过直接优化接受率而不是 KL 散度的 LK 损失来提高推测解码速度，在不同的模型大小和域中显示出一致的性能增益。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>Speculative decoding accelerates autoregressive large language model (LLM) inference by using a lightweight draft model to propose candidate tokens that are then verified in parallel by the target model. The speedup is significantly determined by the acceptance rate, yet standard training minimizes Kullback-Leibler (KL) divergence as a proxy objective. While KL divergence and acceptance rate share the same global optimum, small draft models, having limited capacity, typically converge to suboptimal solutions where minimizing KL does not guarantee maximizing acceptance rate. To address this issue, we propose LK losses, special training objectives that directly target acceptance rate. Comprehensive experiments across four draft architectures and six target models, ranging from 8B to 685B parameters, demonstrate consistent improvements in acceptance metrics across all configurations compared to the standard KL-based training. We evaluate our approach on general, coding and math domains and report gains of up to 8-10% in average acceptance length. LK losses are easy to implement, introduce no computational overhead and can be directly integrated into any existing speculator training framework, making them a compelling alternative to the existing draft training objectives.</p>
  <p>推测性解码通过使用轻量级草稿模型提出候选标记，然后由目标模型并行验证，从而加速自回归大语言模型 (LLM) 推理。加速很大程度上取决于接受率，但标准训练将 Kullback-Leibler (KL) 散度最小化作为代理目标。虽然 KL 散度和接受率具有相同的全局最优值，但容量有限的小型草图模型通常会收敛到次优解决方案，其中最小化 KL 并不能保证最大化接受率。为了解决这个问题，我们提出了 LK 损失，即直接针对接受率的特殊培训目标。跨四种草稿架构和六个目标模型（参数范围从 8B 到 685B）的综合实验表明，与基于 KL 的标准训练相比，所有配置的验收指标都有一致的改进。我们评估了我们在通用、编码和数学领域的方法，并报告平均接受长度提高了 8-10%。 LK 损失易于实现，不会引入计算开销，并且可以直接集成到任何现有的投机者培训框架中，使其成为现有草案培训目标的引人注目的替代方案。</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 10:30:24 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.23881</guid>
    </item>
    <item>
      <title>Memory Caching: RNNs with Growing Memory</title>
      <link>https://huggingface.co/papers/2602.24281</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Ali Behrouz, Zeman Li, Yuan Deng, Peilin Zhong, Meisam Razaviyayn</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.24281">arXiv</a> | <a href="https://arxiv.org/pdf/2602.24281.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract Memory Caching enhances recurrent models by allowing their memory capacity to scale with sequence length, bridging the gap between traditional RNNs and Transformers in long-context tasks. 
					AI-generated summary</p>
  <p>摘要 内存缓存通过允许内存容量随序列长度扩展来增强循环模型，从而弥合了长上下文任务中传统 RNN 和 Transformer 之间的差距。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>Transformers have been established as the de-facto backbones for most recent advances in sequence modeling, mainly due to their growing memory capacity that scales with the context length. While plausible for retrieval tasks, it causes quadratic complexity and so has motivated recent studies to explore viable subquadratic recurrent alternatives. Despite showing promising preliminary results in diverse domains, such recurrent architectures underperform Transformers in recall-intensive tasks, often attributed to their fixed-size memory. In this paper, we introduce Memory Caching (MC), a simple yet effective technique that enhances recurrent models by caching checkpoints of their memory states (a.k.a. hidden states). Memory Caching allows the effective memory capacity of RNNs to grow with sequence length, offering a flexible trade-off that interpolates between the fixed memory (i.e., O(L) complexity) of RNNs and the growing memory (i.e., O(L^2) complexity) of Transformers. We propose four variants of MC, including gated aggregation and sparse selective mechanisms, and discuss their implications on both linear and deep memory modules. Our experimental results on language modeling, and long-context understanding tasks show that MC enhances the performance of recurrent models, supporting its effectiveness. The results of in-context recall tasks indicate that while Transformers achieve the best accuracy, our MC variants show competitive performance, close the gap with Transformers, and performs better than state-of-the-art recurrent models.</p>
  <p>Transformer 已被确立为序列建模最新进展的事实上的支柱，这主要是因为它们的记忆容量随着上下文长度的增长而增长。虽然对于检索任务来说似乎是合理的，但它会导致二次复杂性，因此促使最近的研究探索可行的次二次循环替代方案。尽管在不同领域显示出有希望的初步结果，但这种循环架构在回忆密集型任务中表现不佳，这通常归因于其固定大小的内存。在本文中，我们介绍了内存缓存（MC），这是一种简单而有效的技术，通过缓存内存状态（也称为隐藏状态）的检查点来增强循环模型。内存缓存允许 RNN 的有效内存容量随着序列长度而增长，从而提供了一种灵活的权衡，可以在 RNN 的固定内存（即 O(L) 复杂度）和 Transformer 的增长内存（即 O(L^2) 复杂度）之间进行插值。我们提出了 MC 的四种变体，包括门控聚合和稀疏选择机制，并讨论了它们对线性和深度存储模块的影响。我们在语言建模和长上下文理解任务上的实验结果表明，MC 增强了循环模型的性能，支持了其有效性。上下文回忆任务的结果表明，虽然 Transformers 实现了最佳准确率，但我们的 MC 变体显示出有竞争力的性能，缩小了与 Transformers 的差距，并且比最先进的循环模型表现得更好。</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 03:59:28 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.24281</guid>
    </item>
    <item>
      <title>Accelerating Masked Image Generation by Learning Latent Controlled Dynamics</title>
      <link>https://huggingface.co/papers/2602.23996</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Kaiwen Zhu, Quansheng Zeng, Yuandong Pu, Shuo Cao, Xiaohui Li</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.23996">arXiv</a> | <a href="https://arxiv.org/pdf/2602.23996.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract MIGM-Shortcut accelerates masked image generation by learning a lightweight model that predicts feature evolution velocity, achieving over 4x speedup with maintained quality. 
					AI-generated summary</p>
  <p>摘要 MIGM-Shortcut 通过学习预测特征演化速度的轻量级模型来加速蒙版图像的生成，在保持质量的情况下实现超过 4 倍的加速。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>Masked Image Generation Models (MIGMs) have achieved great success, yet their efficiency is hampered by the multiple steps of bi-directional attention. In fact, there exists notable redundancy in their computation: when sampling discrete tokens, the rich semantics contained in the continuous features are lost. Some existing works attempt to cache the features to approximate future features. However, they exhibit considerable approximation error under aggressive acceleration rates. We attribute this to their limited expressivity and the failure to account for sampling information. To fill this gap, we propose to learn a lightweight model that incorporates both previous features and sampled tokens, and regresses the average velocity field of feature evolution. The model has moderate complexity that suffices to capture the subtle dynamics while keeping lightweight compared to the original base model. We apply our method, MIGM-Shortcut, to two representative MIGM architectures and tasks. In particular, on the state-of-the-art Lumina-DiMOO, it achieves over 4x acceleration of text-to-image generation while maintaining quality, significantly pushing the Pareto frontier of masked image generation. The code and model weights are available at https://github.com/Kaiwen-Zhu/MIGM-Shortcut.</p>
  <p>掩模图像生成模型（MIGM）取得了巨大的成功，但其效率受到双向注意力的多个步骤的阻碍。事实上，它们的计算存在显着的冗余：当对离散标记进行采样时，连续特征中包含的丰富语义会丢失。一些现有的工作尝试缓存特征以近似未来的特征。然而，它们在激进的加速度下表现出相当大的近似误差。我们将此归因于它们的表达能力有限以及未能考虑采样信息。为了填补这一空白，我们建议学习一个轻量级模型，该模型结合了先前的特征和采样的标记，并对特征演化的平均速度场进行回归。该模型具有适度的复杂性，足以捕捉微妙的动态，同时与原始基础模型相比保持轻量级。我们将我们的方法 MIGM-Shortcut 应用于两个代表性的 MIGM 架构和任务。特别是，在最先进的 Lumina-DiMOO 上，它在保持质量的同时实现了超过 4 倍的文本到图像生成加速，显着推动了蒙版图像生成的帕累托前沿。代码和模型权重可在 https://github.com/Kaiwen-Zhu/MIGM-Shortcut 获取。</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 03:59:26 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.23996</guid>
    </item>
    <item>
      <title>CiteAudit: You Cited It, But Did You Read It? A Benchmark for Verifying Scientific References in the LLM Era</title>
      <link>https://huggingface.co/papers/2602.23452</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: University of Notre Dame | Authors: Zhengqing Yuan, Kaiwen Shi, Zheyuan Zhang, Lichao Sun, Nitesh V. Chawla</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.23452">arXiv</a> | <a href="https://arxiv.org/pdf/2602.23452.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract A comprehensive benchmark and detection framework for identifying fabricated citations in scientific writing using a multi-agent verification pipeline that assesses citation faithfulness and evidence alignment. 
					AI-generated summary</p>
  <p>摘要：一种全面的基准和检测框架，用于使用评估引文忠实度和证据一致性的多代理验证管道来识别科学写作中的虚假引文。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>Scientific research relies on accurate citation for attribution and integrity, yet large language models (LLMs) introduce a new risk: fabricated references that appear plausible but correspond to no real publications. Such hallucinated citations have already been observed in submissions and accepted papers at major machine learning venues, exposing vulnerabilities in peer review. Meanwhile, rapidly growing reference lists make manual verification impractical, and existing automated tools remain fragile to noisy and heterogeneous citation formats and lack standardized evaluation. We present the first comprehensive benchmark and detection framework for hallucinated citations in scientific writing. Our multi-agent verification pipeline decomposes citation checking into claim extraction, evidence retrieval, passage matching, reasoning, and calibrated judgment to assess whether a cited source truly supports its claim. We construct a large-scale human-validated dataset across domains and define unified metrics for citation faithfulness and evidence alignment. Experiments with state-of-the-art LLMs reveal substantial citation errors and show that our framework significantly outperforms prior methods in both accuracy and interpretability. This work provides the first scalable infrastructure for auditing citations in the LLM era and practical tools to improve the trustworthiness of scientific references.</p>
  <p>科学研究依赖于准确引用的归属和完整性，但大型语言模型 (LLM) 引入了新的风险：伪造的参考文献看似合理，但与真实的出版物不相符。这种幻觉引用已经在主要机器学习场所的提交和接受的论文中观察到，暴露了同行评审中的漏洞。与此同时，快速增长的参考文献列表使得手动验证变得不切实际，现有的自动化工具仍然容易受到噪音和异构引文格式的影响，并且缺乏标准化的评估。我们提出了第一个针对科学写作中的幻觉引用的综合基准和检测框架。我们的多代理验证管道将引文检查分解为主张提取、证据检索、段落匹配、推理和校准判断，以评估引用来源是否真正支持其主张。我们构建了跨领域的大规模人工验证数据集，并为引文忠实度和证据对齐定义了统一的指标。使用最先进的法学硕士进行的实验揭示了严重的引用错误，并表明我们的框架在准确性和可解释性方面都显着优于先前的方法。这项工作为法学硕士时代的引文审计提供了第一个可扩展的基础设施，并提供了提高科学参考文献可信度的实用工具。</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 06:38:52 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.23452</guid>
    </item>
    <item>
      <title>Mode Seeking meets Mean Seeking for Fast Long Video Generation</title>
      <link>https://huggingface.co/papers/2602.24289</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: NVIDIA | Authors: Shengqu Cai, Weili Nie, Chao Liu, Julius Berner, Lvmin Zhang</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.24289">arXiv</a> | <a href="https://arxiv.org/pdf/2602.24289.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract A training paradigm combining mode seeking and mean seeking in a Decoupled Diffusion Transformer enables efficient generation of high-quality long videos by leveraging both global flow matching and local distribution matching techniques. 
					AI-generated summary</p>
  <p>摘要：在解耦扩散变压器中结合模式搜索和均值搜索的训练范例通过利用全局流匹配和局部分布匹配技术能够有效生成高质量的长视频。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>Scaling video generation from seconds to minutes faces a critical bottleneck: while short-video data is abundant and high-fidelity, coherent long-form data is scarce and limited to narrow domains. To address this, we propose a training paradigm where Mode Seeking meets Mean Seeking, decoupling local fidelity from long-term coherence based on a unified representation via a Decoupled Diffusion Transformer. Our approach utilizes a global Flow Matching head trained via supervised learning on long videos to capture narrative structure, while simultaneously employing a local Distribution Matching head that aligns sliding windows to a frozen short-video teacher via a mode-seeking reverse-KL divergence. This strategy enables the synthesis of minute-scale videos that learns long-range coherence and motions from limited long videos via supervised flow matching, while inheriting local realism by aligning every sliding-window segment of the student to a frozen short-video teacher, resulting in a few-step fast long video generator. Evaluations show that our method effectively closes the fidelity-horizon gap by jointly improving local sharpness, motion and long-range consistency. Project website: https://primecai.github.io/mmm/.</p>
  <p>将视频生成从几秒扩展到几分钟面临着一个关键瓶颈：虽然短视频数据丰富且高保真，但连贯的长格式数据却稀缺且仅限于狭窄的领域。为了解决这个问题，我们提出了一种训练范式，其中模式搜索满足均值搜索，通过解耦扩散变压器基于统一表示将局部保真度与长期一致性解耦。我们的方法利用通过长视频监督学习训练的全局流匹配头来捕获叙事结构，同时采用局部分布匹配头，通过模式搜索反向 KL 散度将滑动窗口与冻结的短视频教师对齐。该策略能够合成分钟尺度的视频，通过监督流匹配从有限的长视频中学习远程连贯性和运动，同时通过将学生的每个滑动窗口片段与冻结的短视频教师对齐来继承局部现实主义，从而产生几步快速的长视频生成器。评估表明，我们的方法通过共同提高局部清晰度、运动和远程一致性，有效地缩小了保真度与视野的差距。项目网站：https://primecai.github.io/mmm/。</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 06:38:50 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.24289</guid>
    </item>
    <item>
      <title>CUDA Agent: Large-Scale Agentic RL for High-Performance CUDA Kernel Generation</title>
      <link>https://huggingface.co/papers/2602.24286</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: ByteDance Seed | Authors: Weinan Dai, Hanlin Wu, Qiying Yu, Huan-ang Gao, Jiahao Li</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.24286">arXiv</a> | <a href="https://arxiv.org/pdf/2602.24286.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract CUDA Agent, a large-scale agentic reinforcement learning system, achieves state-of-the-art performance in CUDA kernel optimization by combining scalable data synthesis, skill-augmented development environment, and reinforcement learning techniques. 
					AI-generated summary</p>
  <p>摘要 CUDA Agent 是一种大规模代理强化学习系统，通过结合可扩展的数据合成、技能增强的开发环境和强化学习技术，在 CUDA 内核优化方面实现了最先进的性能。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>GPU kernel optimization is fundamental to modern deep learning but remains a highly specialized task requiring deep hardware expertise. Despite strong performance in general programming, large language models (LLMs) remain uncompetitive with compiler-based systems such as torch.compile for CUDA kernel generation. Existing CUDA code generation approaches either rely on training-free refinement or fine-tune models within fixed multi-turn execution-feedback loops, but both paradigms fail to fundamentally improve the model's intrinsic CUDA optimization ability, resulting in limited performance gains. We present CUDA Agent, a large-scale agentic reinforcement learning system that develops CUDA kernel expertise through three components: a scalable data synthesis pipeline, a skill-augmented CUDA development environment with automated verification and profiling to provide reliable reward signals, and reinforcement learning algorithmic techniques enabling stable training. CUDA Agent achieves state-of-the-art results on KernelBench, delivering 100\%, 100\%, and 92\% faster rate over torch.compile on KernelBench Level-1, Level-2, and Level-3 splits, outperforming the strongest proprietary models such as Claude Opus 4.5 and Gemini 3 Pro by about 40\% on the hardest Level-3 setting.</p>
  <p>GPU 内核优化是现代深度学习的基础，但仍然是一项高度专业化的任务，需要深厚的硬件专业知识。尽管在通用编程方面具有强大的性能，但大型语言模型 (LLM) 与基于编译器的系统（例如用于 CUDA 内核生成的 torch.compile）相比仍然没有竞争力。现有的 CUDA 代码生成方法要么依赖于免训练细化，要么依赖于固定多轮执行反馈循环内的微调模型，但这两种范式都无法从根本上提高模型内在的 CUDA 优化能力，导致性能提升有限。我们推出了 CUDA Agent，这是一个大规模代理强化学习系统，它通过三个组件开发 CUDA 内核专业知识：可扩展的数据合成管道、具有自动验证和分析功能的技能增强 CUDA 开发环境，以提供可靠的奖励信号，以及支持稳定训练的强化学习算法技术。 CUDA Agent 在 KernelBench 上实现了最先进的结果，在 KernelBench Level-1、Level-2 和 Level-3 分割上比 torch.compile 速度快 100%、100% 和 92%，在最难的 Level-3 设置上比 Claude Opus 4.5 和 Gemini 3 Pro 等最强的专有模型高出约 40%。</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 03:59:27 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.24286</guid>
    </item>
    <item>
      <title>Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets</title>
      <link>https://huggingface.co/papers/2602.22207</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Institute for Computer Science, Artificial intelligence and Technology | Authors: Hanna Yukhymenko, Anton Alexandrov, Martin Vechev</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.22207">arXiv</a> | <a href="https://arxiv.org/pdf/2602.22207.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract An automated framework for high-quality multilingual translation of benchmarks addresses semantic drift and context loss issues, improving LLM evaluation reliability through advanced test-time compute scaling strategies. 
					AI-generated summary</p>
  <p>摘要：用于高质量多语言基准翻译的自动化框架解决了语义漂移和上下文丢失问题，通过先进的测试时计算扩展策略提高了 LLM 评估的可靠性。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development.</p>
  <p>目前，多语言大语言模型 (LLM) 评估的可靠性因翻译基准质量不一致而受到影响。现有资源经常遭受语义漂移和上下文丢失的影响，这可能导致误导性的性能指标。在这项工作中，我们提出了一个完全自动化的框架，旨在通过实现数据集和基准的可扩展、高质量转换来应对这些挑战。我们证明，与传统管道相比，调整测试时计算扩展策略，特别是通用自我改进（USI）和我们提出的多轮排名方法 T-RANK，可以显着提高质量输出。我们的框架确保基准测试在本地化过程中保留其原始任务结构和语言细微差别。我们应用这种方法将流行的基准和数据集翻译成八种东欧和南欧语言（乌克兰语、保加利亚语、斯洛伐克语、罗马尼亚语、立陶宛语、爱沙尼亚语、土耳其语、希腊语）。使用基于参考的指标和法学硕士作为法官进行的评估表明，我们的翻译超越了现有资源，从而实现了更准确的下游模型评估。我们发布了框架和改进的基准，以促进稳健且可重复的多语言人工智能开发。</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 06:38:48 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.22207</guid>
    </item>
    <item>
      <title>Enhancing Spatial Understanding in Image Generation via Reward Modeling</title>
      <link>https://huggingface.co/papers/2602.24233</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Peking University | Authors: Zhenyu Tang, Chaoran Feng, Yufan Deng, Jie Wu, Xiaojie Li</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.24233">arXiv</a> | <a href="https://arxiv.org/pdf/2602.24233.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract A new reward model called SpatialScore is introduced to improve spatial relationship understanding in text-to-image generation through reinforcement learning with a large-scale dataset of preference pairs. 
					AI-generated summary</p>
  <p>摘要 引入了一种名为 SpatialScore 的新奖励模型，通过使用大规模偏好对数据集进行强化学习，提高文本到图像生成中的空间关系理解。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>Recent progress in text-to-image generation has greatly advanced visual fidelity and creativity, but it has also imposed higher demands on prompt complexity-particularly in encoding intricate spatial relationships. In such cases, achieving satisfactory results often requires multiple sampling attempts. To address this challenge, we introduce a novel method that strengthens the spatial understanding of current image generation models. We first construct the SpatialReward-Dataset with over 80k preference pairs. Building on this dataset, we build SpatialScore, a reward model designed to evaluate the accuracy of spatial relationships in text-to-image generation, achieving performance that even surpasses leading proprietary models on spatial evaluation. We further demonstrate that this reward model effectively enables online reinforcement learning for the complex spatial generation. Extensive experiments across multiple benchmarks show that our specialized reward model yields significant and consistent gains in spatial understanding for image generation.</p>
  <p>文本到图像生成的最新进展极大地提高了视觉保真度和创造力，但它也对即时复杂性提出了更高的要求，特别是在编码复杂的空间关系方面。在这种情况下，获得满意的结果通常需要多次采样尝试。为了应对这一挑战，我们引入了一种新方法，可以加强对当前图像生成模型的空间理解。我们首先构建具有超过 80k 偏好对的 SpatialReward 数据集。在此数据集的基础上，我们构建了 SpatialScore，这是一种奖励模型，旨在评估文本到图像生成中空间关系的准确性，其性能甚至超越了领先的空间评估专有模型。我们进一步证明，这种奖励模型有效地实现了复杂空间生成的在线强化学习。跨多个基准的广泛实验表明，我们的专门奖励模型在图像生成的空间理解方面产生了显着且一致的收益。</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 03:59:25 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.24233</guid>
    </item>
    <item>
      <title>dLLM: Simple Diffusion Language Modeling</title>
      <link>https://huggingface.co/papers/2602.22661</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: UC Berkeley | Authors: Zhanhui Zhou, Lingjie Chen, Hanghang Tong, Dawn Song</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2602.22661">arXiv</a> | <a href="https://arxiv.org/pdf/2602.22661.pdf">PDF</a></p>
  <h3>AI summary</h3>
  <p>Abstract A unified open-source framework is presented that standardizes core components of diffusion language modeling for reproduction, customization, and accessible development of both large and small models. 
					AI-generated summary</p>
  <p>摘要 提出了一个统一的开源框架，该框架标准化了扩散语言建模的核心组件，以实现大型和小型模型的复制、定制和可访问开发。 
					AI 生成的摘要</p>
  <h3>Abstract</h3>
  <p>Although diffusion language models (DLMs) are evolving quickly, many recent models converge on a set of shared components. These components, however, are distributed across ad-hoc research codebases or lack transparent implementations, making them difficult to reproduce or extend. As the field accelerates, there is a clear need for a unified framework that standardizes these common components while remaining flexible enough to support new methods and architectures.
  To address this gap, we introduce dLLM, an open-source framework that unifies the core components of diffusion language modeling -- training, inference, and evaluation -- and makes them easy to customize for new designs. With dLLM, users can reproduce, finetune, deploy, and evaluate open-source large DLMs such as LLaDA and Dream through a standardized pipeline. The framework also provides minimal, reproducible recipes for building small DLMs from scratch with accessible compute, including converting any BERT-style encoder or autoregressive LM into a DLM. We also release the checkpoints of these small DLMs to make DLMs more accessible and accelerate future research.</p>
  <p>尽管扩散语言模型 (DLM) 发展迅速，但许多最新模型都集中在一组共享组件上。然而，这些组件分布在临时研究代码库中或缺乏透明的实现，使得它们难以复制或扩展。随着该领域的发展，显然需要一个统一的框架来标准化这些通用组件，同时保持足够的灵活性以支持新的方法和架构。
  为了弥补这一差距，我们引入了 dLLM，这是一个开源框架，它统一了扩散语言建模的核心组件（训练、推理和评估），并使它们易于针对新设计​​进行定制。借助 dLLM，用户可以通过标准化管道重现、微调、部署和评估 LLaDA 和 Dream 等开源大型 DLM。该框架还提供了最少的、可重复的方法，用于从头开始构建具有可访问计算的小型 DLM，包括将任何 BERT 式编码器或自回归 LM 转换为 DLM。我们还发布了这些小型 DLM 的检查点，以使 DLM 更易于访问并加速未来的研究。</p>
</div>]]></description>
      <pubDate>Mon, 02 Mar 2026 06:38:47 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2602.22661</guid>
    </item>
  </channel>
</rss>
