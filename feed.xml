<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hugging Face Papers RSS</title>
    <link>https://huggingface.co/papers</link>
    <description>Latest AI research papers from Hugging Face</description>
    <lastBuildDate>Sat, 31 Jan 2026 12:55:10 GMT</lastBuildDate>
    <atom:link href="https://AzureSilent.github.io/hf-paper-rss/feed.xml" rel="self" type="application/rss+xml"/>
    <item>
      <title>STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation</title>
      <link>https://huggingface.co/papers/2601.20381</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Alexandre Chapin, Emmanuel Dellandréa, Liming Chen</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.20381">arXiv</a> | <a href="https://arxiv.org/pdf/2601.20381.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	STORM enhances robotic manipulation by adapting visual foundation models with semantic-aware slots through multi-phase training, improving generalization and control performance.
				
					AI-generated summary</p>
  <p>摘要
	STORM 通过多阶段训练将视觉基础模型与语义感知插槽相适应，从而增强机器人操作，从而提高泛化和控制性能。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.</p>
  <p>视觉基础模型为机器人技术提供了强大的感知特征，但它们的密集表示缺乏明确的对象级结构，限制了操作任务的鲁棒性和收缩性。我们提出了 STORM（用于机器人操作的基于槽的任务感知对象中心表示），这是一个轻量级的以对象为中心的适应模块，它通过一小组用于机器人操作的语义感知槽来增强冻结的视觉基础模型。 STORM 没有重新训练大型骨干网，而是采用多阶段训练策略：首先通过使用语言嵌入的视觉语义预训练来稳定以对象为中心的槽，然后与下游操纵策略联合调整。这种分阶段学习可以防止退化槽形成并保持语义一致性，同时使感知与任务目标保持一致。对象发现基准和模拟操作任务的实验表明，与直接使用冻结基础模型特征或端到端训练以对象为中心的表示相比，STORM 提高了对视觉干扰因素的泛化和控制性能。我们的结果强调多阶段适应是一种有效的机制，用于将通用基础模型特征转换为机器人控制的任务感知的以对象为中心的表示。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:10 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.20381</guid>
    </item>
    <item>
      <title>WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models</title>
      <link>https://huggingface.co/papers/2601.21282</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Rishi Upadhyay, Howard Zhang, Jim Solomon, Ayush Agrawal, Pranay Boreddy</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21282">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21282.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	WorldBench is introduced as a video-based benchmark for disentangled evaluation of physical reasoning in generative models, revealing specific failure patterns in current state-of-the-art video world models.
				
					AI-generated summary</p>
  <p>摘要
	WorldBench 是作为基于视频的基准引入的，用于对生成模型中的物理推理进行解耦评估，揭示当前最先进的视频世界模型中的特定故障模式。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Recent advances in generative foundational models, often termed "world models," have propelled interest in applying them to critical tasks like robotic planning and autonomous system training. For reliable deployment, these models must exhibit high physical fidelity, accurately simulating real-world dynamics. Existing physics-based video benchmarks, however, suffer from entanglement, where a single test simultaneously evaluates multiple physical laws and concepts, fundamentally limiting their diagnostic capability. We introduce WorldBench, a novel video-based benchmark specifically designed for concept-specific, disentangled evaluation, allowing us to rigorously isolate and assess understanding of a single physical concept or law at a time. To make WorldBench comprehensive, we design benchmarks at two different levels: 1) an evaluation of intuitive physical understanding with concepts such as object permanence or scale/perspective, and 2) an evaluation of low-level physical constants and material properties such as friction coefficients or fluid viscosity. When SOTA video-based world models are evaluated on WorldBench, we find specific patterns of failure in particular physics concepts, with all tested models lacking the physical consistency required to generate reliable real-world interactions. Through its concept-specific evaluation, WorldBench offers a more nuanced and scalable framework for rigorously evaluating the physical reasoning capabilities of video generation and world models, paving the way for more robust and generalizable world-model-driven learning.</p>
  <p>生成基础模型（通常被称为“世界模型”）的最新进展激发了人们将其应用于机器人规划和自主系统训练等关键任务的兴趣。为了实现可靠的部署，这些模型必须表现出高物理保真度，准确模拟现实世界的动态。然而，现有的基于物理的视频基准测试存在纠缠问题，其中单个测试同时评估多个物理定律和概念，从根本上限制了它们的诊断能力。我们推出了 WorldBench，这是一种新颖的基于视频的基准测试，专为特定概念、解开的评估而设计，使我们能够一次严格隔离和评估对单个物理概念或定律的理解。为了使 WorldBench 更全面，我们设计了两个不同级别的基准：1) 使用物体持久性或尺度/视角等概念来评估直观的物理理解，2) 对低级物理常数和材料属性（例如摩擦系数或流体粘度）进行评估。当在 WorldBench 上评估基于 SOTA 视频的世界模型时，我们发现特定物理概念中的特定失败模式，所有测试的模型都缺乏生成可靠的现实世界交互所需的物理一致性。通过针对具体概念的评估，WorldBench 提供了一个更加细致和可扩展的框架，用于严格评估视频生成和世界模型的物理推理能力，为更强大和更通用的世界模型驱动学习铺平道路。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:10 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21282</guid>
    </item>
    <item>
      <title>Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation</title>
      <link>https://huggingface.co/papers/2601.21416</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Alexandre Chapin, Bruno Machado, Emmanuel Dellandréa, Liming Chen</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21416">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21416.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Slot-Based Object-Centric Representations outperform global and dense feature representations in robotic manipulation tasks by providing better generalization under visual distribution shifts.
				
					AI-generated summary</p>
  <p>摘要
	基于槽的以对象为中心的表示通过在视觉分布变化下提供更好的泛化能力，在机器人操作任务中优于全局和密集的特征表示。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.</p>
  <p>机器人操作策略的泛化能力很大程度上受到视觉表示的选择的影响。现有方法通常依赖于从预先训练的编码器中提取的表示，使用两种主要类型的特征：全局特征（通过单个池化向量总结整个图像）和密集特征（保留最终编码器层的补丁式嵌入）。虽然广泛使用，但这两种特征类型混合了与任务相关和不相关的信息，导致在分布变化（例如照明、纹理的变化或干扰物的存在）下泛化能力较差。在这项工作中，我们探索了一种中间结构化替代方案：基于槽的以对象为中心的表示（SBOCR），它将密集特征分组为一组有限的类对象实体。这种表示允许自然地减少提供给机器人操作策略的噪声，同时保留足够的信息以有效地执行任务。我们将一系列全局和密集表示与基于中间槽的表示进行基准测试，涵盖一系列从简单到复杂的模拟和现实世界操作任务。我们评估它们在不同视觉条件下的概括，包括照明、纹理的变化和干扰物的存在。我们的研究结果表明，即使没有针对特定任务的预训练，基于 SBOCR 的策略在泛化环境中也优于基于密集和全局表示的策略。这些见解表明，SBOCR 是设计视觉系统的一个有前途的方向，可以在动态、真实的机器人环境中有效地泛化。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:10 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21416</guid>
    </item>
    <item>
      <title>WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents</title>
      <link>https://huggingface.co/papers/2601.21872</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Ludwig Maximilian University of Munich | Authors: Yao Zhang, Shijie Tang, Zeyu Li, Zhen Han, Volker Tresp</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21872">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21872.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	WebArbiter introduces a reasoning-first WebPRM that formulates reward modeling as text generation to improve web navigation through structured justifications and preference verdicts, outperforming existing baselines in complex web environments.
				
					AI-generated summary</p>
  <p>摘要
	WebArbiter 引入了推理优先的 WebPRM，它将奖励模型制定为文本生成，通过结构化理由和偏好判决来改进 Web 导航，在复杂的 Web 环境中超越现有基线。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.</p>
  <p>网络代理在自动化复杂计算机任务方面具有巨大潜力，但它们的交互涉及长期、连续的决策和不可逆转的行为。在这种情况下，基于结果的监督是稀疏且延迟的，通常奖励不正确的轨迹并且无法支持推理时间缩放。这促使人们使用流程奖励模型 (WebPRM) 进行 Web 导航，但现有方法仍然有限：标量 WebPRM 将进度分解为粗略、弱接地信号，而基于清单的 WebPRM 依赖于脆弱的模板匹配，在布局或语义更改下会失败，并且经常将表面上正确的操作错误地标记为成功，几乎无法提供洞察力或可解释性。为了应对这些挑战，我们引入了 WebArbiter，这是一种推理优先、原则归纳的 WebPRM，它将奖励模型制定为文本生成，生成结构化的理由，以偏好判决结束，并确定在当前环境下最有利于任务完成的操作。训练遵循两阶段流程：推理蒸馏为模型配备了连贯的原则引导推理，强化学习通过直接将判决与正确性对齐来纠正教师的偏见，从而实现更强的泛化。为了支持系统评估，我们发布了 WebPRMBench，这是一个涵盖四种不同 Web 环境的综合基准测试，具有丰富的任务和高质量的偏好注释。在 WebPRMBench 上，WebArbiter-7B 的性能比最强的基线 GPT-5 高出 9.1 分。在 WebArena-Lite 上的奖励引导轨迹搜索中，它超越了之前最好的 WebPRM 高达 7.2 个点，强调了它在现实世界复杂 Web 任务中的鲁棒性和实用价值。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:10 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21872</guid>
    </item>
    <item>
      <title>PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement</title>
      <link>https://huggingface.co/papers/2601.11747</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Huaxiaoyue Wang, Sunav Choudhary, Franck Dernoncourt, Yu Shen, Stefano Petrangeli</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.11747">arXiv</a> | <a href="https://arxiv.org/pdf/2601.11747.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	PRISM leverages design data to create a knowledge base for improving graphic designs based on natural language instructions, achieving superior style alignment compared to existing methods.
				
					AI-generated summary</p>
  <p>摘要
	PRISM 利用设计数据创建一个知识库，用于基于自然语言指令改进图形设计，与现有方法相比，实现卓越的风格对齐。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Graphic design often involves exploring different stylistic directions, which can be time-consuming for non-experts. We address this problem of stylistically improving designs based on natural language instructions. While VLMs have shown initial success in graphic design, their pretrained knowledge on styles is often too general and misaligned with specific domain data. For example, VLMs may associate minimalism with abstract designs, whereas designers emphasize shape and color choices. Our key insight is to leverage design data -- a collection of real-world designs that implicitly capture designer's principles -- to learn design knowledge and guide stylistic improvement. We propose PRISM (PRior-Informed Stylistic Modification) that constructs and applies a design knowledge base through three stages: (1) clustering high-variance designs to capture diversity within a style, (2) summarizing each cluster into actionable design knowledge, and (3) retrieving relevant knowledge during inference to enable style-aware improvement. Experiments on the Crello dataset show that PRISM achieves the highest average rank of 1.49 (closer to 1 is better) over baselines in style alignment. User studies further validate these results, showing that PRISM is consistently preferred by designers.</p>
  <p>平面设计通常涉及探索不同的风格方向，这对于非专家来说可能非常耗时。我们解决了基于自然语言指令在风格上改进设计的问题。虽然 VLM 在图形设计方面取得了初步成功，但它们预先训练的样式知识往往过于笼统，并且与特定领域数据不一致。例如，VLM 可能会将极简主义与抽象设计联系起来，而设计师则强调形状和颜色的选择。我们的主要见解是利用设计数据（隐含地捕捉设计师原则的现实设计的集合）来学习设计知识并指导风格改进。我们提出 PRISM（PRIor-Informed Stylistic Modification），通过三个阶段构建和应用设计知识库：（1）对高方差设计进行聚类以捕获风格内的多样性，（2）将每个聚类总结为可操作的设计知识，以及（3）在推理过程中检索相关知识以实现风格感知改进。 Crello 数据集上的实验表明，PRISM 在风格对齐方面比基线达到了最高平均排名 1.49（越接近 1 越好）。用户研究进一步验证了这些结果，表明 PRISM 始终受到设计师的青睐。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:10 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.11747</guid>
    </item>
    <item>
      <title>Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance</title>
      <link>https://huggingface.co/papers/2601.17690</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Ziling Gong, Yunyan Ouyang, Iram Kamdar, Melody Ma, Hongjie Chen</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.17690">arXiv</a> | <a href="https://arxiv.org/pdf/2601.17690.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Neural audio fingerprinting performance varies with segment length, with short segments (0.5-second) generally providing better retrieval accuracy, and large language models showing promise in recommending optimal segment durations.
				
					AI-generated summary</p>
  <p>摘要
	神经音频指纹识别性能随片段长度的不同而变化，短片段（0.5 秒）通常可以提供更好的检索精度，而大型语言模型在推荐最佳片段持续时间方面表现出良好的前景。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems.</p>
  <p>音频指纹提供了声学信号的可识别表示，稍后可用于识别和检索系统。为了获得有区别的表示，输入音频通常被分割成更短的时间间隔，从而允许提取和分析局部声学特征。现代神经方法通常对短的、固定持续时间的音频片段进行操作，但片段持续时间的选择通常是启发式的，很少深入检查。在本文中，我们研究了片段长度如何影响音频指纹识别性能。我们扩展了现有的神经指纹识别架构，以采用各种片段长度并评估不同片段长度和查询持续时间的检索准确性。我们的结果表明，较短的段长度（0.5 秒）通常可以获得更好的性能。此外，我们评估了 LLM 推荐最佳片段长度的能力，这表明 GPT-5-mini 在三个研究的 LLM 中始终针对五个考虑因素给出了最佳建议。我们的研究结果为在大规模神经音频检索系统中选择片段持续时间提供了实用指导。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:09 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.17690</guid>
    </item>
    <item>
      <title>Flow-based Extremal Mathematical Structure Discovery</title>
      <link>https://huggingface.co/papers/2601.18005</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Center for Scalable Data Analytics and Artificial Intelligence Dresden/Leipzig | Authors: Gergely Bérczi, Baran Hashemi, Jonas Klüver</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.18005">arXiv</a> | <a href="https://arxiv.org/pdf/2601.18005.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	FlowBoost is a closed-loop generative framework that combines geometry-aware flow-matching, reward-guided policy optimization, and stochastic local search to efficiently discover extremal geometric structures with improved results over existing methods.
				
					AI-generated summary</p>
  <p>摘要
	FlowBoost 是一个闭环生成框架，结合了几何感知流匹配、奖励引导策略优化和随机局部搜索，可有效发现极值几何结构，并比现有方法改进结果。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>The discovery of extremal structures in mathematics requires navigating vast and nonconvex landscapes where analytical methods offer little guidance and brute-force search becomes intractable. We introduce FlowBoost, a closed-loop generative framework that learns to discover rare and extremal geometric structures by combining three components: (i) a geometry-aware conditional flow-matching model that learns to sample high-quality configurations, (ii) reward-guided policy optimization with action exploration that directly optimizes the generation process toward the objective while maintaining diversity, and (iii) stochastic local search for both training-data generation and final refinement. Unlike prior open-loop approaches, such as PatternBoost that retrains on filtered discrete samples, or AlphaEvolve which relies on frozen Large Language Models (LLMs) as evolutionary mutation operators, FlowBoost enforces geometric feasibility during sampling, and propagates reward signal directly into the generative model, closing the optimization loop and requiring much smaller training sets and shorter training times, and reducing the required outer-loop iterations by orders of magnitude, while eliminating dependence on LLMs. We demonstrate the framework on four geometric optimization problems: sphere packing in hypercubes, circle packing maximizing sum of radii, the Heilbronn triangle problem, and star discrepancy minimization. In several cases, FlowBoost discovers configurations that match or exceed the best known results. For circle packings, we improve the best known lower bounds, surpassing the LLM-based system AlphaEvolve while using substantially fewer computational resources.</p>
  <p>数学中极值结构的发现需要探索广阔的非凸景观，在这些景观中，分析方法几乎无法提供指导，暴力搜索也变得棘手。我们介绍了 FlowBoost，一个闭环生成框架，它通过组合三个组件来学习发现罕见和极值的几何结构：（i）学习对高质量配置进行采样的几何感知条件流匹配模型，（ii）带有行动探索的奖励引导策略优化，直接优化生成过程以实现目标，同时保持多样性，以及（iii）用于训练数据生成和最终细化的随机局部搜索。与之前的开环方法不同，例如在过滤后的离散样本上重新训练的 PatternBoost 或依靠冻结的大型语言模型 (LLM) 作为进化变异算子的 AlphaEvolve，FlowBoost 在采样过程中强制执行几何可行性，并将奖励信号直接传播到生成模型中，关闭优化循环并需要更小的训练集和更短的训练时间，并按数量级减少所需的外循环迭代，同时消除对 LLM 的依赖。我们演示了四个几何优化问题的框架：超立方体中的球体堆积、半径总和最大化的圆堆积、海尔布隆三角形问题和星形差异最小化。在某些情况下，FlowBoost 会发现匹配或超过已知结果的配置。对于圆形填充，我们改进了最著名的下界，超越了基于 LLM 的系统 AlphaEvolve，同时使用更少的计算资源。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:09 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.18005</guid>
    </item>
    <item>
      <title>Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis</title>
      <link>https://huggingface.co/papers/2601.20103</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Patronus AI | Authors: Darshan Deshpande, Anand Kannappan, Rebecca Qian</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.20103">arXiv</a> | <a href="https://arxiv.org/pdf/2601.20103.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Researchers developed a comprehensive benchmark for detecting reward hacking in code generation environments, demonstrating that contrastive anomaly detection outperforms isolated classification approaches and revealing challenges with semantically contextualized reward hacks.
				
					AI-generated summary</p>
  <p>摘要
	研究人员开发了一个全面的基准，用于检测代码生成环境中的奖励黑客行为，证明对比异常检测优于孤立的分类方法，并揭示了语义上下文奖励黑客行为的挑战。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.</p>
  <p>代码生成强化学习的最新进展使得强大的环境对于防止奖励黑客攻击至关重要。随着法学硕士越来越多地充当基于代码的强化学习的评估者，他们检测奖励黑客的能力仍然没有得到充分研究。在本文中，我们提出了一种涵盖 54 个类别的奖励利用的新颖分类法，并介绍了 TRACE（在代码环境中测试奖励异常），这是一个综合策划和人工验证的基准，包含 517 个测试轨迹。与之前在孤立分类场景中评估奖励黑客检测的工作不同，我们将这些评估与 TRACE 上更现实、对比异常检测设置进行对比。我们的实验表明，模型在对比设置中比在孤立分类设置中更有效地捕获奖励黑客，具有最高推理模式的 GPT-5.2 实现了 63% 的最佳检测率，高于 TRACE 上孤立设置中的 45%。基于这一见解，我们证明，与句法语境化的模型相比，最先进的模型在语义语境化的奖励黑客方面的困难要大得多。我们进一步对模型行为进行定性分析，以及消融研究，表明良性轨迹与被黑轨迹的比率以及分析簇大小会极大地影响检测性能。我们发布基准和评估工具，使社区能够扩展 TRACE 并评估他们的模型。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:09 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.20103</guid>
    </item>
    <item>
      <title>Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels</title>
      <link>https://huggingface.co/papers/2601.21268</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Micah Rentschler, Jesse Roberts</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21268">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21268.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Reinforcement Learning from Meta-Evaluation optimizes language model generators using rewards from evaluators' judgments on natural-language meta-questions, enabling training without ground-truth labels while achieving comparable accuracy and sample efficiency.
				
					AI-generated summary</p>
  <p>摘要
	元评估的强化学习使用评估者对自然语言元问题的判断的奖励来优化语言模型生成器，从而无需真实标签即可进行训练，同时实现可比较的准确性和样本效率。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Most reinforcement learning (RL) methods for training large language models (LLMs) require ground-truth labels or task-specific verifiers, limiting scalability when correctness is ambiguous or expensive to obtain. We introduce Reinforcement Learning from Meta-Evaluation (RLME), which optimizes a generator using reward derived from an evaluator's answers to natural-language meta-questions (e.g., "Is the answer correct?" or "Is the reasoning logically consistent?"). RLME treats the evaluator's probability of a positive judgment as a reward and updates the generator via group-relative policy optimization, enabling learning without labels. Across a suite of experiments, we show that RLME achieves accuracy and sample efficiency comparable to label-based training, enables controllable trade-offs among multiple objectives, steers models toward reliable reasoning patterns rather than post-hoc rationalization, and generalizes to open-domain settings where ground-truth labels are unavailable, broadening the domains in which LLMs may be trained with RL.</p>
  <p>大多数用于训练大型语言模型 (LLM) 的强化学习 (RL) 方法都需要真实标签或特定于任务的验证器，当正确性不明确或获取成本昂贵时，会限制可扩展性。我们引入了元评估强化学习（RLME），它使用从评估者对自然语言元问题的答案中获得的奖励来优化生成器（例如，“答案正确吗？”或“推理在逻辑上是否一致？”）。 RLME 将评估者做出积极判断的概率视为奖励，并通过群体相关策略优化来更新生成器，从而实现无标签学习。通过一系列实验，我们表明 RLME 实现了与基于标签的训练相当的准确性和样本效率，实现了多个目标之间的可控权衡，引导模型走向可靠的推理模式而不是事后合理化，并推广到无法获得真实标签的开放域设置，从而拓宽了法学硕士可以使用 RL 进行训练的领域。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:09 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21268</guid>
    </item>
    <item>
      <title>FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning</title>
      <link>https://huggingface.co/papers/2601.19001</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Northwestern University | Authors: Haozheng Luo, Zhuolin Jiang, Md Zahid Hasan, Yan Chen, Soumalya Sarkar</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.19001">arXiv</a> | <a href="https://arxiv.org/pdf/2601.19001.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	FROST is an attention-aware method that improves reasoning efficiency by pruning uncritical paths and removing reasoning outliers, leading to reduced token usage and improved accuracy.
				
					AI-generated summary</p>
  <p>摘要
	FROST 是一种注意力感知方法，通过修剪非关键路径和删除推理异常值来提高推理效率，从而减少令牌使用并提高准确性。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>We propose FROST, an attention-aware method for efficient reasoning. Unlike traditional approaches, FROST leverages attention weights to prune uncritical reasoning paths, yielding shorter and more reliable reasoning trajectories. Methodologically, we introduce the concept of reasoning outliers and design an attention-based mechanism to remove them. Theoretically, FROST preserves and enhances the model's reasoning capacity while eliminating outliers at the sentence level. Empirically, we validate FROST on four benchmarks using two strong reasoning models (Phi-4-Reasoning and GPT-OSS-20B), outperforming state-of-the-art methods such as TALE and ThinkLess. Notably, FROST achieves an average 69.68% reduction in token usage and a 26.70% improvement in accuracy over the base model. Furthermore, in evaluations of attention outlier metrics, FROST reduces the maximum infinity norm by 15.97% and the average kurtosis by 91.09% compared to the base model. Code is available at https://github.com/robinzixuan/FROST</p>
  <p>我们提出了 FROST，一种用于高效推理的注意力感知方法。与传统方法不同，FROST 利用注意力权重来修剪不加批判的推理路径，产生更短、更可靠的推理轨迹。在方法上，我们引入了推理异常值的概念，并设计了一种基于注意力的机制来消除它们。理论上，FROST 保留并增强了模型的推理能力，同时消除了句子级别的异常值。根据经验，我们使用两个强大的推理模型（Phi-4-Reasoning 和 GPT-OSS-20B）在四个基准上验证 FROST，其性能优于 TALE 和 ThinkLess 等最先进的方法。值得注意的是，与基本模型相比，FROST 的代币使用量平均减少了 69.68%，准确性提高了 26.70%。此外，在注意力异常值指标的评估中，与基础模型相比，FROST 将最大无穷范数降低了 15.97%，将平均峰度降低了 91.09%。代码可在 https://github.com/robinzixuan/FROST 获取</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:09 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.19001</guid>
    </item>
    <item>
      <title>JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion</title>
      <link>https://huggingface.co/papers/2601.22143</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.22143">arXiv</a> | <a href="https://arxiv.org/pdf/2601.22143.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	A lightweight LoRA adaptation of an audio-video diffusion model enables high-quality video dubbing with preserved speaker identity and improved lip synchronization through synthetic multilingual video training.
				
					AI-generated summary</p>
  <p>摘要
	音视频扩散模型的轻量级 LoRA 改编可实现高质量视频配音，并通过合成多语言视频训练保留说话者身份并改进唇形同步。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.</p>
  <p>经过预先训练以联合生成声音和视觉内容的视听基础模型最近显示出前所未有的多模式生成和编辑建模能力，为下游任务开辟了新的机会。在这些任务中，视频配音可以从这些先验中受益匪浅，但大多数现有解决方案仍然依赖于复杂的、特定于任务的管道，而这些管道在现实环境中举步维艰。在这项工作中，我们引入了一种单模型方法，该方法通过轻量级 LoRA 采用基础音视频扩散模型进行视频到视频配音。 LoRA 使模型能够根据输入音频-视频进行调节，同时联合生成翻译后的音频和同步的面部运动。为了训练这个 LoRA，我们利用生成模型本身来合成同一说话者的配对多语言视频。具体来说，我们在单个剪辑中生成带有语言切换的多语言视频，然后修复每一半的面部和音频以匹配另一半的语言。通过利用视听模型丰富的生成先验，我们的方法保留了说话者身份和唇形同步，同时对复杂的运动和现实世界的动态保持鲁棒性。我们证明，与现有的配音流程相比，我们的方法可以生成高质量的配音视频，具有更高的视觉保真度、口型同步和鲁棒性。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:09 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.22143</guid>
    </item>
    <item>
      <title>BMAM: Brain-inspired Multi-Agent Memory Framework</title>
      <link>https://huggingface.co/papers/2601.20465</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Yang Li, Jiaxiang Liu, Yusong Wang, Yujie Wu, Mingkun Xu</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.20465">arXiv</a> | <a href="https://arxiv.org/pdf/2601.20465.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	BMAM presents a brain-inspired multi-agent memory architecture that decomposes memory into specialized subsystems to address long-term reasoning challenges in language-model-based agents.
				
					AI-generated summary</p>
  <p>摘要
	BMAM 提出了一种受大脑启发的多智能体记忆架构，它将记忆分解为专门的子系统，以解决基于语言模型的智能体的长期推理挑战。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.</p>
  <p>在扩展交互范围内运行的基于语言模型的代理在保留临时信息和保持跨会话行为一致性方面面临着持续的挑战，我们将这种失败模式称为灵魂侵蚀。我们提出了 BMAM（脑启发多智能体内存），这是一种通用内存架构，它将智能体内存建模为一组功能专用的子系统，而不是单个非结构化存储。受认知记忆系统的启发，BMAM 将记忆分解为情景、语义、显着性感知和控制导向的组件，这些组件在互补的时间尺度上运行。为了支持长视野推理，BMAM 沿着明确的时间线组织情景记忆，并通过融合多个互补信号来检索证据。 LoCoMo 基准测试表明，BMAM 在标准长视野评估设置下达到了 78.45% 的准确率，消融分析证实，受海马体启发的情景记忆子系统在时间推理中发挥着关键作用。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:09 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.20465</guid>
    </item>
    <item>
      <title>Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation</title>
      <link>https://huggingface.co/papers/2601.21406</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Zihan Su, Hongyang Wei, Kangrui Cen, Yong Wang, Guanhua Chen</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21406">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21406.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	UniMRG enhances unified multimodal models by training them to generate multiple visual representations, improving both understanding and generation capabilities through complementary information capture.
				
					AI-generated summary</p>
  <p>摘要
	UniMRG 通过训练生成多种视觉表示来增强统一的多模态模型，通过补充信息捕获提高理解和生成能力。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Unified Multimodal Models (UMMs) integrate both visual understanding and generation within a single framework. Their ultimate aspiration is to create a cycle where understanding and generation mutually reinforce each other. While recent post-training methods have successfully leveraged understanding to enhance generation, the reverse direction of utilizing generation to improve understanding remains largely unexplored. In this work, we propose UniMRG (Unified Multi-Representation Generation), a simple yet effective architecture-agnostic post-training method. UniMRG enhances the understanding capabilities of UMMs by incorporating auxiliary generation tasks. Specifically, we train UMMs to generate multiple intrinsic representations of input images, namely pixel (reconstruction), depth (geometry), and segmentation (structure), alongside standard visual understanding objectives. By synthesizing these diverse representations, UMMs capture complementary information regarding appearance, spatial relations, and structural layout. Consequently, UMMs develop a deeper and more comprehensive understanding of visual inputs. Extensive experiments across diverse UMM architectures demonstrate that our method notably enhances fine-grained perception, reduces hallucinations, and improves spatial understanding, while simultaneously boosting generation capabilities.</p>
  <p>统一多模态模型 (UMM) 将视觉理解和生成集成在一个框架内。他们的最终愿望是创​​造一个理解与生成相互促进的循环。虽然最近的训练后方法已经成功地利用理解来增强生成，但利用生成来提高理解的反向方向在很大程度上仍未被探索。在这项工作中，我们提出了 UniMRG（统一多重表示生成），这是一种简单而有效的与架构无关的后训练方法。 UniMRG 通过合并辅助生成任务来增强 UMM 的理解能力。具体来说，我们训练 UMM 生成输入图像的多种内在表示，即像素（重建）、深度（几何）和分割（结构），以及标准视觉理解目标。通过综合这些不同的表示，UMM 捕获有关外观、空间关系和结构布局的补充信息。因此，UMM 对视觉输入有了更深入、更全面的理解。跨不同 UMM 架构的大量实验表明，我们的方法显着增强了细粒度感知、减少幻觉并提高空间理解，同时提高生成能力。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:09 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21406</guid>
    </item>
    <item>
      <title>Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units</title>
      <link>https://huggingface.co/papers/2601.21996</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Peking University | Authors: Jianhui Chen, Yuzhang Luo, Liangming Pan</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21996">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21996.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Mechnistic Data Attribution framework traces interpretable units to specific training samples using influence functions, demonstrating causal relationships between data structure and neural circuit formation in language models.
				
					AI-generated summary</p>
  <p>摘要
	机械数据归因框架使用影响函数将可解释单元追溯到特定的训练样本，证明语言模型中数据结构和神经回路形成之间的因果关系。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.</p>
  <p>虽然机械可解释性已经确定了法学硕士中的可解释电路，但它们在训练数据中的因果起源仍然难以捉摸。我们引入了机械数据归因（MDA），这是一个可扩展的框架，它利用影响函数将可解释的单元追溯到特定的训练样本。通过对 Pythia 家族进行大量实验，我们因果性地验证了有针对性的干预（删除或增加一小部分高影响力样本）可以显着调节可解释头部的出现，而随机干预则没有效果。我们的分析表明，重复的结构数据（例如 LaTeX、XML）起到了机械催化剂的作用。此外，我们观察到针对诱导头部形成的干预措施会导致模型的情境学习（ICL）能力同时发生变化。这为关于感应头和 ICL 之间功能联系的长期假设提供了直接的因果证据。最后，我们提出了一种机械数据增强管道，可以持续加速跨模型尺度的电路收敛，为指导法学硕士的发展轨迹提供原则性方法。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:09 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21996</guid>
    </item>
    <item>
      <title>MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</title>
      <link>https://huggingface.co/papers/2601.22054</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Baorui Ma, Jiahui Yang, Donglin Di, Xuancheng Zhang, Jianxun Cui</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.22054">arXiv</a> | <a href="https://arxiv.org/pdf/2601.22054.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Metric Anything presents a scalable pretraining framework for metric depth estimation that leverages diverse 3D data and sparse metric prompts to achieve superior performance across multiple vision tasks.
				
					AI-generated summary</p>
  <p>摘要
	Metric Anything 提供了一个用于度量深度估计的可扩展预训练框架，该框架利用不同的 3D 数据和稀疏度量提示在多个视觉任务中实现卓越的性能。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.</p>
  <p>缩放推动了视觉基础模型的最新进展，但由于异构传感器噪声、相机相关偏差以及嘈杂的跨源 3D 数据中的度量模糊性，将这种范式扩展到度量深度估计仍然具有挑战性。我们推出了 Metric Anything，这是一个简单且可扩展的预训练框架，可以从嘈杂、多样化的 3D 源中学习度量深度，而无需手动设计提示、特定于相机的建模或特定于任务的架构。我们方法的核心是稀疏度量提示，它是通过随机屏蔽深度图创建的，它作为一个通用接口，将空间推理与传感器和相机偏差分离。使用跨越 10000 个相机模型重建、捕获和渲染 3D 数据的约 2000 万个图像深度对，我们首次展示了公制深度轨道中清晰的缩放趋势。预训练模型擅长提示驱动任务，例如深度完成、超分辨率和雷达相机融合，而其精炼的无提示学生在单目深度估计、相机内在恢复、单/多视图度量 3D 重建和 VLA 规划方面取得了最先进的结果。我们还表明，使用 Metric Anything 的预训练 ViT 作为视觉编码器可以显着增强空间智能中的多模态大语言模型能力。这些结果表明，度量深度估计可以受益于驱动现代基础模型的相同缩放法则，从而建立一条通向可扩展且高效的现实世界度量感知的新路径。我们在 http://metric-anything.github.io/metric-anything-io/ 上开源了 MetricAnything 以支持社区研究。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:09 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.22054</guid>
    </item>
    <item>
      <title>ECO: Quantized Training without Full-Precision Master Weights</title>
      <link>https://huggingface.co/papers/2601.22101</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Google | Authors: Mahdi Nikdan, Amir Zandieh, Dan Alistarh, Vahab Mirrokni</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.22101">arXiv</a> | <a href="https://arxiv.org/pdf/2601.22101.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Error-compensating optimizer eliminates memory overhead from master weights in quantized LLM training while maintaining near-lossless accuracy.
				
					AI-generated summary</p>
  <p>摘要
	误差补偿优化器消除了量化 LLM 训练中主权重的内存开销，同时保持近乎无损的准确性。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as master weights. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier.</p>
  <p>量化显着提高了大型语言模型 (LLM) 训练的计算和内存效率。然而，现有的方法仍然依赖于高精度累积更新：具体来说，梯度更新必须应用于高精度权重缓冲区，称为主权重。此缓冲区引入了大量的内存开销，特别是对于稀疏专家混合 (SMoE) 模型，其中模型参数和优化器状态主导内存使用。为了解决这个问题，我们引入了误差补偿优化器（ECO），它通过直接对量化参数应用更新来消除主权重。 ECO 在每一步之后量化权重，并小心地将产生的量化误差注入优化器动量中，形成一个没有额外内存的误差反馈循环。我们证明，在标准假设和衰减的学习率下，ECO 收敛到最优值的恒定半径邻域，而朴素的主权重去除可能会产生与学习率成反比的误差。我们展示了预训练小型 Transformer (30-800M)、Gemma-3 1B 模型和具有 FP8 量化的 2.1B 参数稀疏 MoE 模型以及以 INT4 精度微调 DeepSeek-MoE-16B 的实证结果。在整个过程中，ECO 将基线与主权重进行匹配，达到近乎无损的精度，显着改变了静态记忆与验证损失的帕累托边界。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:09 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.22101</guid>
    </item>
    <item>
      <title>KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices</title>
      <link>https://huggingface.co/papers/2601.21579</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Wuyang Zhou, Yuxuan Gu, Giorgos Iacovides, Danilo Mandic</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21579">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21579.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	KromHC addresses training instability and scalability issues in hyper-connections by using Kronecker products to parametrize residual matrices with reduced parameter complexity.
				
					AI-generated summary</p>
  <p>摘要
	KromHC 通过使用 Kronecker 产品来参数化残差矩阵并降低参数复杂性，解决了超连接中的训练不稳定和可扩展性问题。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive O(n^3C) parameter complexity with n as the width of the residual stream and C as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, O left( nC cdot n! right). To address both challenges, we propose KromHC, which uses the Kronecker products of smaller doubly stochastic matrices to parametrize the residual matrix in mHC. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to O(n^2C). Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at https://github.com/wz1119/KromHC.</p>
  <p>超连接（HC）在神经网络（NN）中的成功也凸显了与其训练不稳定和可扩展性受限相关的问题。流形约束超连接 (mHC) 通过将残差连接空间投影到 Birkhoff 多胞体上来缓解这些挑战，但是，它面临两个问题：1) 其迭代 Sinkhorn-Knopp (SK) 算法并不总是产生精确的双随机残差矩阵； 2) mHC 会产生令人望而却步的 O(n^3C) 参数复杂度，其中 n 为残差流的宽度，C 为特征维度。最近提出的 mHC-lite 通过 Birkhoff-von-Neumann 定理重新参数化残差矩阵以保证双随机性，但也面临着参数复杂度的阶乘爆炸，O left( nC cdot n! right)。为了解决这两个挑战，我们提出了 KromHC，它使用较小的双随机矩阵的克罗内克乘积来参数化 mHC 中的残差矩阵。通过沿着张量化残差流的每个模式对因子残差矩阵施加流形约束，KromHC 保证了残差矩阵的精确双随机性，同时将参数复杂度降低到 O(n^2C)。综合实验表明，KromHC 可以匹配甚至超越最先进的 (SOTA) mHC 变体，同时需要的可训练参数显着减少。该代码可从 https://github.com/wz1119/KromHC 获取。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:08 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21579</guid>
    </item>
    <item>
      <title>FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale</title>
      <link>https://huggingface.co/papers/2601.22146</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: FineInstructions | Authors: Ajay Patel, Colin Raffel, Chris Callison-Burch</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.22146">arXiv</a> | <a href="https://arxiv.org/pdf/2601.22146.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Large language models can be pre-trained from scratch using synthetic instruction-response pairs generated from unstructured text corpora, outperforming traditional methods on benchmarks measuring response quality.
				
					AI-generated summary</p>
  <p>摘要
	大型语言模型可以使用从非结构化文本语料库生成的合成指令-响应对从头开始进行预训练，在测量响应质量的基准上优于传统方法。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised "predict the next word" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of "instruction-tuning" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With "supervised" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .</p>
  <p>由于监督训练数据有限，大型语言模型 (LLM) 通常通过自我监督的“预测下一个单词”目标对大量非结构化文本数据进行预训练。为了使生成的模型对用户有用，它在由指令和响应的监督训练示例组成的少量“指令调整”数据上进行了进一步训练。为了克服监督数据量有限的问题，我们提出了一种程序，可以将互联网规模的预训练文档中的知识转化为数十亿个合成指令和答案训练对。生成的数据集称为 FineInstructions，使用根据真实用户编写的查询和提示创建的约 18M 指令模板。这些指令模板与来自非结构化预训练语料库的人工编写的源文档相匹配并实例化。通过以这种规模生成的“监督”合成训练数据，LLM 可以仅通过指令调整目标从头开始进行预训练，这与 LLM 的预期下游使用（响应用户提示）更加不分布。我们进行了受控的逐令牌训练实验，发现 FineInstructions 上的预训练优于标准预训练和其他在测量自由形式响应质量的标准基准上提出的综合预训练技术。我们的资源可以在 https://huggingface.co/fineinstructions 找到。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:08 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.22146</guid>
    </item>
    <item>
      <title>Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts</title>
      <link>https://huggingface.co/papers/2601.22156</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: OpenBMB | Authors: Yingfa Chen, Zhen Leng Thai, Zihan Zhou, Zhu Zhang, Xingyu Shen</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.22156">arXiv</a> | <a href="https://arxiv.org/pdf/2601.22156.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	HALO enables efficient conversion of Transformer models to RNN-attention hybrid architectures with improved long-context performance using minimal training data.
				
					AI-generated summary</p>
  <p>摘要
	HALO 能够使用最少的训练数据将 Transformer 模型高效转换为 RNN-attention 混合架构，并提高长上下文性能。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data</p>
  <p>混合 Transformer 架构结合了 softmax 注意力模块和循环神经网络 (RNN)，在长上下文建模中表现出了理想的性能-吞吐量权衡，但它们的采用和研究却因从头开始进行大规模预训练的高昂成本而受到阻碍。最近的一些研究表明，预训练的 softmax 注意力块可以通过参数传递和知识蒸馏转换为 RNN 块。然而，这些传输方法需要大量的训练数据（超过 10B 个 token），并且由此产生的混合模型也表现出较差的长上下文性能，在这种情况下，混合模型比基于 Transformer 的模型享有显着的推理加速。在本文中，我们提出了 HALO（通过层优化的混合注意力），这是一种将 Transformer 模型提炼为 RNN-注意力混合模型的管道。然后，我们提出了 HypeNet，这是一种具有卓越长度泛化能力的混合架构，通过新颖的位置编码方案（称为 HyPE）和各种架构修改实现。我们使用 HALO 将 Qwen3 系列转换为 HypeNet，实现与原始 Transformer 模型相当的性能，同时享受卓越的长上下文性能和效率。转换仅需要 2.3B 代币，不到预训练数据的 0.01%</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:08 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.22156</guid>
    </item>
    <item>
      <title>DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents</title>
      <link>https://huggingface.co/papers/2601.20975</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Google | Authors: Nikita Gupta, Riju Chatterjee, Lukas Haas, Connie Tao, Andrew Wang</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.20975">arXiv</a> | <a href="https://arxiv.org/pdf/2601.20975.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	DeepSearchQA presents a 900-prompt benchmark evaluating agents on complex multi-step information-seeking tasks requiring systematic information collation, deduplication, and reasoning about stopping criteria across 17 fields.
				
					AI-generated summary</p>
  <p>摘要
	DeepSearchQA 提供了一个包含 900 个提示的基准测试，用于评估智能体执行复杂的多步骤信息搜索任务的能力，这些任务需要系统性的信息整理、重复数据删除以及对 17 个领域的停止标准进行推理。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>We introduce DeepSearchQA, a 900-prompt benchmark for evaluating agents on difficult multi-step information-seeking tasks across 17 different fields. Unlike traditional benchmarks that target single answer retrieval or broad-spectrum factuality, DeepSearchQA features a dataset of challenging, handcrafted tasks designed to evaluate an agent's ability to execute complex search plans to generate exhaustive answer lists. This shift in design explicitly tests three critical, yet under-evaluated capabilities: 1) systematic collation of fragmented information from disparate sources, 2) de-duplication and entity resolution to ensure precision, and 3) the ability to reason about stopping criteria within an open-ended search space. Each task is structured as a causal chain, where discovering information for one step is dependent on the successful completion of the previous one, stressing long-horizon planning and context retention. All tasks are grounded in the open web with objectively verifiable answer sets. Our comprehensive evaluation of state-of-the-art agent architectures reveals significant performance limitations: even the most advanced models struggle to balance high recall with precision. We observe distinct failure modes ranging from premature stopping (under-retrieval) to hedging behaviors, where agents cast an overly wide net of low-confidence answers to artificially boost recall. These findings highlight critical headroom in current agent designs and position DeepSearchQA as an essential diagnostic tool for driving future research toward more robust, deep-research capabilities.</p>
  <p>我们推出了 DeepSearchQA，这是一个包含 900 条提示的基准，用于评估代理在 17 个不同领域执行困难的多步骤信息搜索任务。与针对单一答案检索或广泛事实性的传统基准不同，DeepSearchQA 具有具有挑战性的手工任务数据集，旨在评估代理执行复杂搜索计划以生成详尽答案列表的能力。这种设计上的转变明确地测试了三个关键但未被充分评估的功能：1）对来自不同来源的碎片信息进行系统整理，2）重复数据删除和实体解析以确保精度，以及3）在开放式搜索空间内推理停止标准的能力。每项任务都被构造为一个因果链，其中一个步骤的信息发现取决于前一个步骤的成功完成，强调长期规划和上下文保留。所有任务都基于开放网络，具有客观可验证的答案集。我们对最先进的代理架构的综合评估揭示了显着的性能限制：即使是最先进的模型也难以平衡高召回率和精确度。我们观察到不同的失败模式，从过早停止（检索不足）到对冲行为，其中代理撒下一张过于宽泛的低置信度答案网，人为地提高召回率。这些发现凸显了当前代理设计中的关键空间，并将 DeepSearchQA 定位为推动未来研究走向更强大、更深入的研究能力的重要诊断工具。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:08 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.20975</guid>
    </item>
    <item>
      <title>Self-Improving Pretraining: using post-trained models to pretrain better models</title>
      <link>https://huggingface.co/papers/2601.21343</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Meta Research | Authors: Ellen Xiaoqing Tan, Shehzaad Dhuliawala, Jing Xu, Ping Yu, Sainbayar Sukhbaatar</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21343">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21343.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	A reinforcement learning-based pretraining method improves language model safety, factuality, and quality by evaluating generations through a combination of model rollouts, original suffixes, and rewritten suffixes.
				
					AI-generated summary</p>
  <p>摘要
	基于强化学习的预训练方法通过模型推出、原始后缀和重写后缀的组合来评估各代，从而提高语言模型的安全性、真实性和质量。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.</p>
  <p>确保一代又一代大型语言模型的安全性、真实性和整体质量是一项严峻的挑战，特别是当这些模型越来越多地部署在现实世界的应用程序中时。解决这些问题的主流方法包括收集昂贵的、精心策划的数据集，并应用多个阶段的微调和对齐。然而，即使这个复杂的管道也不能保证预训练期间学到的模式的正确性。因此，在预训练期间解决这些问题至关重要，因为它塑造了模型的核心行为，并防止不安全或幻觉的输出被深深嵌入。为了解决这个问题，我们引入了一种新的预训练方法，该方法可以流式传输文档并使用强化学习 (RL) 来改进每一步生成的下一个 K 个标记。一个强大的、经过训练的模型会判断候选代的质量、安全性和真实性，包括模型的推出、原始后缀和重写的后缀。在训练早期，该过程依赖于原始和重写的后缀；随着模型的改进，强化学习会奖励高质量的推出。这种方法从头开始构建更高质量、更安全、更真实的模型。在实验中，我们的方法在真实性和安全性方面比标准预训练提高了 36.2% 和 18.5%，并且在整体生成质量方面提高了高达 86.3% 的获胜率。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:08 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21343</guid>
    </item>
    <item>
      <title>Beyond Imitation: Reinforcement Learning for Active Latent Planning</title>
      <link>https://huggingface.co/papers/2601.21598</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Zhi Zheng, Wee Sun Lee</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21598">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21598.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Active latent planning method improves reasoning accuracy and efficiency by modeling latent token supervision as conditional VAE and using reinforcement learning with coherence rewards.
				
					AI-generated summary</p>
  <p>摘要
	主动潜在规划方法通过将潜在令牌监督建模为条件 VAE 并使用具有一致性奖励的强化学习来提高推理准确性和效率。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the Active Latent Planning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.</p>
  <p>针对高效、密集的思想链 (CoT) 推理，潜在推理方法对大型语言模型 (LLM) 进行微调，以用连续的潜在标记替代离散的语言标记。与传统语言 CoT 推理相比，这些方法消耗更少的 token，并且有可能在密集的潜在空间中进行规划。然而，当前的潜在标记通常是基于模仿语言标签来进行监督的。考虑到一个问题可能存在多个等效但不同的 CoT 标签，被动地模仿任意一个可能会导致较差的潜在令牌表示和潜在推理策略，从而破坏潜在的规划能力并导致训练和测试之间出现明显的差距。在这项工作中，我们强调了对潜在令牌的表示空间进行主动规划对于实现最佳潜在推理策略的重要性。因此，我们提出了主动潜在规划方法（ATP-Latent），它将潜在令牌的监督过程建模为条件变分自动编码器（VAE），以获得更平滑的潜在空间。此外，为了促进最合理的潜在推理策略，ATP-Latent 使用辅助一致性奖励进行强化学习（RL），该奖励是根据潜在令牌的 VAE 解码内容之间的一致性计算的，从而实现引导的 RL 过程。在 LLaMA-1B 的实验中，与高级基线相比，ATP-Latent 在四个基准测试中显示出 +4.1\% 的准确度和 -3.3\% 的标记。代码可在 https://github.com/zz1358m/ATP-Latent-master 上找到。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:08 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21598</guid>
    </item>
    <item>
      <title>One-step Latent-free Image Generation with Pixel Mean Flows</title>
      <link>https://huggingface.co/papers/2601.22158</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Yiyang Lu, Susie Lu, Qiao Sun, Hanhong Zhao, Zhicheng Jiang</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.22158">arXiv</a> | <a href="https://arxiv.org/pdf/2601.22158.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Pixel MeanFlow introduces a one-step latent-free image generation method by separating network output space from loss space, achieving strong performance on ImageNet at multiple resolutions.
				
					AI-generated summary</p>
  <p>摘要
	Pixel MeanFlow 通过将网络输出空间与损失空间分离，引入了一种一步式无潜在图像生成方法，在多种分辨率下在 ImageNet 上实现了强大的性能。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.</p>
  <p>用于图像生成的现代基于扩散/流的模型通常表现出两个核心特征：（i）使用多步采样，以及（ii）在潜在空间中操作。最近的进展在每个方面都取得了令人鼓舞的进展，为无潜伏的一步扩散/流动铺平了道路。在这项工作中，我们朝着这个目标又迈出了一步，提出了“像素平均流”（pMF）。我们的核心准则是分别制定网络输出空间和损失空间。网络目标被设计为位于假定的低维图像流形（即 x 预测）上，而损失是通过速度空间中的 MeanFlow 定义的。我们引入图像流形和平均速度场之间的简单变换。在实验中，pMF 在 ImageNet 上以 256x256 分辨率（2.22 FID）和 512x512 分辨率（2.48 FID）实现单步潜在无生成，取得了很好的结果，填补了这一领域的关键缺失。我们希望我们的研究将进一步推进基于扩散/流的生成模型的边界。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:08 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.22158</guid>
    </item>
    <item>
      <title>MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models</title>
      <link>https://huggingface.co/papers/2601.21181</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Sangyun Chung, Se Yeon Kim, Youngchae Chee, Yong Man Ro</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21181">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21181.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Multimodal Large Language Models suffer from cross-modal hallucinations where one modality incorrectly influences generation from another, leading to fabricated outputs; this exposes a fundamental deficiency in modality-interaction control. To address this, a training-free method called Modality-Adaptive Decoding (MAD) is proposed that adaptively weights modality-specific decoding branches based on task requirements by leveraging the model's inherent ability to self-assess modality relevance. MAD uses extracted modality probabilities to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models, showing that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning.
				
					AI-generated summary</p>
  <p>摘要
	多模态大语言模型遭受跨模态幻觉，其中一种模态错误地影响另一种模态的生成，导致伪造的输出；这暴露了模态交互控制的根本缺陷。为了解决这个问题，提出了一种称为模态自适应解码（MAD）的免训练方法，该方法通过利用模型自我评估模态相关性的固有能力，根据任务要求自适应地对特定模态的解码分支进行加权。 MAD 使用提取的模态概率对对比解码分支进行自适应加权，使模型能够专注于相关信息，同时抑制跨模态干扰。 CMM 和 AVHBench 上的大量实验表明，MAD 显着减少了多个视听语言模型的跨模态幻觉，表明通过自我评估的明确模态意识对于稳健的多模态推理至关重要。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\% and 2.0\% improvements for VideoLLaMA2-AV, 8.7\% and 4.7\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at https://github.com/top-yun/MAD{https://github.com/top-yun/MAD}</p>
  <p>多模态大语言模型（MLLM）遭受跨模态幻觉的困扰，其中一种模态不恰当地影响另一种模态的生成，从而导致伪造的输出。这暴露了模态交互控制中更根本的缺陷。为了解决这个问题，我们提出了模态自适应解码（MAD），这是一种无需训练的方法，可以根据任务要求自适应地对特定模态的解码分支进行加权。 MAD 利用模型固有的能力，通过查询每个任务需要哪些模态来自我评估模态相关性。然后使用提取的模态概率对对比解码分支进行自适应加权，使模型能够专注于相关信息，同时抑制跨模态干扰。在 CMM 和 AVHBench 上的大量实验表明，MAD 显着减少了跨多种视听语言模型的跨模态幻觉（VideoLLaMA2-AV 分别提高了 7.8\% 和 2.0\%，Qwen2.5-Omni 分别提高了 8.7\% 和 4.7\%）。我们的方法表明，通过自我评估的明确模态意识对于稳健的多模态推理至关重要，为现有的对比解码方法提供了原则性的扩展。我们的代码可以在 https://github.com/top-yun/MAD{https://github.com/top-yun/MAD}</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:08 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21181</guid>
    </item>
    <item>
      <title>VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning</title>
      <link>https://huggingface.co/papers/2601.22069</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Nanyang Technological University | Authors: Yibo Wang, Yongcheng Jing, Shunyu Liu, Hao Guan, Rong-cheng Tu</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.22069">arXiv</a> | <a href="https://arxiv.org/pdf/2601.22069.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	VTC-R1 enables efficient long-context reasoning by compressing textual traces into compact images and iteratively feeding them back into vision-language models as optical memory, achieving significant speedup without sacrificing performance.
				
					AI-generated summary</p>
  <p>摘要
	VTC-R1 通过将文本痕迹压缩为紧凑的图像并将其作为光学存储器迭代反馈到视觉语言模型中，从而实现高效的长上下文推理，从而在不牺牲性能的情况下实现显着的加速。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity. Existing efficient approaches often rely on complex additional training or external models for compression, which limits scalability and discards critical fine-grained information. In this paper, we propose VTC-R1, a new efficient reasoning paradigm that integrates vision-text compression into the reasoning process. Instead of processing lengthy textual traces, VTC-R1 renders intermediate reasoning segments into compact images, which are iteratively fed back into vision-language models as "optical memory." We construct a training dataset based on OpenR1-Math-220K achieving 3.4x token compression and fine-tune representative VLMs-Glyph and Qwen3-VL. Extensive experiments on benchmarks such as MATH500, AIME25, AMC23 and GPQA-D demonstrate that VTC-R1 consistently outperforms standard long-context reasoning. Furthermore, our approach significantly improves inference efficiency, achieving 2.7x speedup in end-to-end latency, highlighting its potential as a scalable solution for reasoning-intensive applications. Our code is available at https://github.com/w-yibo/VTC-R1.</p>
  <p>长上下文推理极大地增强了大型语言模型（LLM）处理复杂任务的能力，但由于计算复杂性，它引入了严重的效率瓶颈。现有的有效方法通常依赖于复杂的额外训练或外部模型进行压缩，这限制了可扩展性并丢弃了关键的细粒度信息。在本文中，我们提出了 VTC-R1，一种新的高效推理范式，它将视觉文本压缩集成到推理过程中。 VTC-R1 不是处理冗长的文本痕迹，而是将中间推理片段渲染成紧凑的图像，这些图像作为“光学记忆”迭代反馈到视觉语言模型中。我们基于 OpenR1-Math-220K 构建了一个训练数据集，实现了 3.4 倍的令牌压缩，并对代表性的 VLM-Glyph 和 Qwen3-VL 进行了微调。对 MATH500、AIME25、AMC23 和 GPQA-D 等基准测试的大量实验表明，VTC-R1 始终优于标准的长上下文推理。此外，我们的方法显着提高了推理效率，在端到端延迟方面实现了 2.7 倍的加速，凸显了其作为推理密集型应用程序的可扩展解决方案的潜力。我们的代码可在 https://github.com/w-yibo/VTC-R1 获取。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:08 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.22069</guid>
    </item>
    <item>
      <title>Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models</title>
      <link>https://huggingface.co/papers/2601.18129</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Typhoon | Authors: Kunat Pipatanakul, Pittawat Taveekitworachai</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.18129">arXiv</a> | <a href="https://arxiv.org/pdf/2601.18129.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	A minimal post-training approach using supervised fine-tuning, on-policy distillation, and small-scale reinforcement fine-tuning enables the development of high-quality sovereign language models with reduced resource requirements.
				
					AI-generated summary</p>
  <p>摘要
	使用监督微调、策略蒸馏和小规模强化微调的最小训练后方法可以在减少资源需求的情况下开发高质量的主权语言模型。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.</p>
  <p>大型语言模型（LLM）进展迅速；然而，大多数最先进的模型主要使用英语和中文等高资源语言进行训练和评估，并且通常由少数能够访问大规模计算和数据的组织开发。这种把关为主权环境造成了实际障碍，在这种环境中，区域或国家规模的机构或领域所有者必须保留对模型权重、训练数据和部署的控制和理解，同时在有限的资源和严格的透明度约束下运行。为此，我们确定了两个核心要求：（1）可采用性，将基础模型转变为通用助手的能力；（2）主权能力，执行高风险、特定区域任务的能力（例如，用当地语言和文化知识进行法律推理）。我们研究是否可以在不扩展大规模指令语料库或依赖复杂的偏好调整管道和大规模强化微调（RFT）的情况下实现这些要求。我们推出了 Typhoon S，这是一个最小且开放的训练后配方，结合了监督微调、策略蒸馏和小规模 RFT。使用 Thai 作为代表性案例研究，我们证明了我们的方法将主权适应和通用基础模型转变为具有强大通用性能的指令调整模型。我们进一步表明，使用 InK-GRPO 的小规模 RFT（GRPO 的扩展，通过下一个单词预测损失来增强 GRPO 损失）可以改善泰国法律推理和泰国特定知识，同时保留一般能力。我们的结果表明，精心设计的培训后策略可以减少所需的教学数据和计算规模，为在学术规模资源下获得高质量主权法学硕士提供了一条实用途径。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:08 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.18129</guid>
    </item>
    <item>
      <title>Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report</title>
      <link>https://huggingface.co/papers/2601.21051</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Cisco Foundation AI | Authors: Zhuoran Yang, Ed Li, Jianliang He, Aman Priyanshu, Baturay Saglam</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21051">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21051.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	A two-stage trained cybersecurity reasoning model achieves competitive performance on specialized tasks while maintaining general capabilities through supervised fine-tuning and reinforcement learning from verifiable rewards.
				
					AI-generated summary</p>
  <p>摘要
	两阶段训练的网络安全推理模型可以在专门任务上实现有竞争力的表现，同时通过监督微调和来自可验证奖励的强化学习来保持一般能力。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.</p>
  <p>我们推出 Foundation-Sec-8B-Reasoning，这是第一个用于网络安全的开源本机推理模型。该模型基于我们之前发布的 Foundation-Sec-8B 基础模型（源自 Llama-3.1-8B-Base），通过结合监督微调 (SFT) 和可验证奖励强化学习 (RLVR) 的两阶段过程进行训练。我们的培训利用涵盖网络安全分析、指令遵循和数学推理的专有推理数据。对 10 个网络安全基准和 10 个通用基准的评估表明，在网络安全任务上，其性能可与更大的模型相媲美，同时保持强大的通用能力。该模型在部署适当的系统提示和护栏时，显示出对多跳推理任务的有效泛化和强大的安全性能。这项工作表明，领域专业推理模型可以在专业任务上实现强大的性能，同时保持广泛的通用能力。我们在 https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning 上公开发布该模型。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:08 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21051</guid>
    </item>
    <item>
      <title>Latent Adversarial Regularization for Offline Preference Optimization</title>
      <link>https://huggingface.co/papers/2601.22083</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Stanford University | Authors: Enyi Jiang, Yibo Jacky Zhang, Yinglun Xu, Andreas Haupt, Nancy Amato</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.22083">arXiv</a> | <a href="https://arxiv.org/pdf/2601.22083.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	GANPO uses latent-space regularization through adversarial divergence minimization to improve language model preference optimization, offering more robust structural feedback than token-level methods.
				
					AI-generated summary</p>
  <p>摘要
	GANPO 通过对抗性分歧最小化使用潜在空间正则化来改进语言模型偏好优化，提供比标记级方法更强大的结构反馈。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.</p>
  <p>从人类反馈中学习通常依赖于偏好优化，通过令牌级正则化来限制策略更新。然而，语言模型的偏好优化特别具有挑战性，因为标记空间相似性并不意味着语义或行为相似性。为了应对这一挑战，我们利用潜在空间正则化来进行语言模型偏好优化。我们引入了 GANPO，它通过惩罚策略模型和参考模型的内部表示之间的分歧来实现潜在空间正则化。鉴于潜在表示与显式概率密度无关，我们采用受 GAN 启发的对抗方法来最小化潜在空间分歧。我们将 GANPO 作为正则器集成到现有的离线偏好优化目标中。跨多个模型架构和任务的实验表明，潜在空间正则化取得了一致的改进。此外，通过将 GANPO 引起的推理偏差与令牌级正则化产生的推理偏差进行比较，我们发现 GANPO 在分布偏移和噪声下提供了更稳健的结构反馈，同时以较小的计算开销保持了可比较的下游性能。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:08 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.22083</guid>
    </item>
    <item>
      <title>Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening</title>
      <link>https://huggingface.co/papers/2601.21590</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou Ammar</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21590">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21590.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	A theoretically grounded method for improving large language model reasoning performance through distribution sharpening without iterative sampling or external rewards, achieving comparable results to reinforcement learning post-training with significantly reduced computational costs.
				
					AI-generated summary</p>
  <p>摘要
	一种基于理论的方法，通过分布锐化来提高大型语言模型推理性能，无需迭代采样或外部奖励，在显着降低计算成本的情况下实现与强化学习后训练相当的结果。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.</p>
  <p>强化学习 (RL) 后训练是提高大型语言模型 (LLM) 推理性能的主要方法，但越来越多的证据表明，其收益主要来自分布锐化，而不是获得新能力。最近的工作表明，使用马尔可夫链蒙特卡罗（MCMC）从 LLM 的功率分布中采样可以在不依赖外部奖励的情况下恢复与 RL 训练后相当的性能；然而，MCMC 的高计算成本使得这种方法无法广泛采用。在这项工作中，我们提出了一种基于理论的替代方案，消除了迭代 MCMC 的需要。我们推导出一种新颖的公式，表明全局功率分布可以通过令牌级缩放的低温功率分布来近似，其中缩放因子捕获未来的轨迹质量。利用这种洞察力，我们引入了一种免训练和免验证的算法，该算法可以自回归锐化基本模型的生成分布。根据经验，我们在四个法学硕士的数学、QA 和代码任务上评估了我们的方法，并表明我们的方法在不依赖任何外部奖励的情况下匹配或超过了一次性 GRPO，同时与基于 MCMC 的采样相比，推理延迟减少了 10 倍以上。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:08 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21590</guid>
    </item>
    <item>
      <title>Shaping capabilities with token-level data filtering</title>
      <link>https://huggingface.co/papers/2601.21571</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Anthropic | Authors: Neil Rathi, Alec Radford</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21571">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21571.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Token filtering during pretraining effectively reduces unwanted language model capabilities while maintaining alignment, becoming more effective at larger scales and tolerating noisy labels with sufficient compute.
				
					AI-generated summary</p>
  <p>摘要
	预训练期间的标记过滤有效地减少了不需要的语言模型功能，同时保持对齐，在更大的规模上变得更加有效，并通过足够的计算容忍嘈杂的标签。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Current approaches to reducing undesired capabilities in language models are largely post hoc, and can thus be easily bypassed by adversaries. A natural alternative is to shape capabilities during pretraining itself. On the proxy task of removing medical capabilities, we show that the simple intervention of filtering pretraining data is highly effective, robust, and inexpensive at scale. Inspired by work on data attribution, we show that filtering tokens is more effective than filtering documents, achieving the same hit to undesired capabilities at a lower cost to benign ones. Training models spanning two orders of magnitude, we then demonstrate that filtering gets more effective with scale: for our largest models, token filtering leads to a 7000x compute slowdown on the forget domain. We also show that models trained with token filtering can still be aligned on the forget domain. Along the way, we introduce a methodology for labeling tokens with sparse autoencoders and distilling cheap, high-quality classifiers. We also demonstrate that filtering can be robust to noisy labels with sufficient pretraining compute.</p>
  <p>当前减少语言模型中不需要的功能的方法很大程度上是事后的，因此很容易被对手绕过。一个自然的替代方案是在预训练期间塑造能力。在消除医疗能力的代理任务上，我们证明过滤预训练数据的简单干预在规模上是高效、稳健且廉价的。受数据归因工作的启发，我们证明过滤标记比过滤文档更有效，以更低的成本实现对不良功能的相同打击。训练跨越两个数量级的模型，然后我们证明过滤随着规模的扩大而变得更加有效：对于我们最大的模型，令牌过滤导致遗忘域上的计算速度减慢 7000 倍。我们还表明，使用标记过滤训练的模型仍然可以在遗忘域上对齐。在此过程中，我们引入了一种使用稀疏自动编码器标记标记并提取廉价、高质量分类器的方法。我们还证明，通过足够的预训练计算，过滤可以对噪声标签具有鲁棒性。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:08 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21571</guid>
    </item>
    <item>
      <title>Discovering Hidden Gems in Model Repositories</title>
      <link>https://huggingface.co/papers/2601.22157</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Jonathan Kahana, Eliahu Horwitz, Yedid Hoshen</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.22157">arXiv</a> | <a href="https://arxiv.org/pdf/2601.22157.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Hidden superior models exist in public repositories but are overlooked due to inefficient discovery methods; a multi-armed bandit approach using shared query sets and aggressive elimination significantly accelerates identification of top-performing models.
				
					AI-generated summary</p>
  <p>摘要
	公共存储库中存在隐藏的高级模型，但由于发现方法效率低下而被忽视；使用共享查询集和积极消除的多臂老虎机方法显着加速了性能最佳模型的识别。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of "hidden gems", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.</p>
  <p>公共存储库托管着数百万个经过微调的模型，但社区使用仍然不成比例地集中在少数基础检查点上。我们调查这种集中是否反映了有效的市场选择，或者是否系统性地忽视了优秀的模型。通过对 2,000 多个模型的广泛评估，我们展示了“隐藏的宝石”的流行，即不受欢迎的微调，但其性能明显优于流行的同行。值得注意的是，在 Llama-3.1-8B 系列中，我们发现很少下载的检查点可以将数学性能从 83.2% 提高到 96.0%，而不会增加推理成本。然而，通过对每个上传的模型进行详尽的评估来发现这些模型在计算上是不可行的。因此，我们将模型发现制定为多臂强盗问题，并通过使用共享查询集和积极的消除计划来加速顺序减半搜索算法。我们的方法检索顶级模型时，每个候选者只需 50 个查询，从而将发现速度提高了 50 倍以上。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:07 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.22157</guid>
    </item>
    <item>
      <title>Language-based Trial and Error Falls Behind in the Era of Experience</title>
      <link>https://huggingface.co/papers/2601.21754</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Nanyang Technological University | Authors: Haoyu Wang, Guozheng Ma, Shugang Cui, Yilun Kong, Haotian Luo</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21754">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21754.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	A novel framework called SCOUT is introduced that uses lightweight scouts to reduce exploration costs for large language models in nonlinguistic environments, enabling improved performance through supervised fine-tuning and reinforcement learning.
				
					AI-generated summary</p>
  <p>摘要
	引入了一种名为 SCOUT 的新颖框架，该框架使用轻量级侦察兵来降低非语言环境中大型语言模型的探索成本，从而通过监督微调和强化学习来提高性能。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight "scouts" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.</p>
  <p>虽然大型语言模型（LLM）在基于语言的代理任务中表现出色，但它们对看不见的非语言环境（例如符号或空间任务）的适用性仍然有限。之前的工作将这种性能差距归因于预训练分布和测试分布之间的不匹配。在这项工作中，我们证明了主要的瓶颈是探索成本过高：掌握这些任务需要大量的试错，这对于在高维语义空间中运行的参数较多的法学硕士来说在计算上是不可持续的。为了解决这个问题，我们提出了 SCOUT（Sub-Scale Collaboration On Unseen Tasks），这是一个将探索与利用分离的新颖框架。我们采用轻量级“侦察兵”（例如小型 MLP）来探测环境动态，其速度和规模远远超过法学硕士。收集到的轨迹用于通过监督微调 (SFT) 引导 LLM，然后通过多轮强化学习 (RL) 来激活其潜在的世界知识。根据经验，SCOUT 使 Qwen2.5-3B-Instruct 模型的平均得分达到 0.86，显着优于专有模型，包括 Gemini-2.5-Pro (0.60)，同时节省约 60% 的 GPU 小时消耗。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:07 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21754</guid>
    </item>
    <item>
      <title>LoL: Longer than Longer, Scaling Video Generation to Hour</title>
      <link>https://huggingface.co/papers/2601.16914</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.16914">arXiv</a> | <a href="https://arxiv.org/pdf/2601.16914.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Researchers developed a method to overcome sink-collapse in autoregressive video generation by addressing the conflict between Rotary Position Embedding and multi-head attention mechanisms, enabling real-time streaming of videos up to 12 hours long.
				
					AI-generated summary</p>
  <p>摘要
	研究人员开发了一种方法，通过解决旋转位置嵌入和多头注意力机制之间的冲突来克服自回归视频生成中的水槽崩溃，从而实现长达 12 小时的实时视频流。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink frame, resulting in abrupt scene resets and cyclic motion patterns. Our analysis reveals that sink-collapse originates from an inherent conflict between the periodic structure of Rotary Position Embedding (RoPE) and the multi-head attention mechanisms prevalent in current generative models. To address it, we propose a lightweight, training-free approach that effectively suppresses this behavior by introducing multi-head RoPE jitter that breaks inter-head attention homogenization and mitigates long-horizon collapse. Extensive experiments show that our method successfully alleviates sink-collapse while preserving generation quality. To the best of our knowledge, this work achieves the first demonstration of real-time, streaming, and infinite-length video generation with little quality decay. As an illustration of this robustness, we generate continuous videos up to 12 hours in length, which, to our knowledge, is among the longest publicly demonstrated results in streaming video generation.</p>
  <p>最近对长格式视频生成的研究已经从双向模型转向自回归模型，但这些方法通常会遭受错误积累和长期一致性丧失的困扰。虽然引入注意力接收器帧来减轻这种性能下降，但它们通常会引发一种严重的故障模式，我们称之为接收器崩溃：生成的内容反复恢复到接收器帧，导致突然的场景重置和循环运动模式。我们的分析表明，汇崩溃源于旋转位置嵌入（RoPE）的周期结构与当前生成模型中普遍存在的多头注意力机制之间的固有冲突。为了解决这个问题，我们提出了一种轻量级、免训练的方法，通过引入多头 RoPE 抖动来有效抑制这种行为，打破头间注意力均匀化并减轻长期崩溃。大量的实验表明，我们的方法成功地缓解了汇崩溃，同时保持了发电质量。据我们所知，这项工作首次实现了实时、流媒体和无限长度视频生成的演示，并且质量几乎没有下降。为了说明这种鲁棒性，我们生成了长达 12 小时的连续视频，据我们所知，这是流媒体视频生成中公开展示的最长的结果之一。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:07 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.16914</guid>
    </item>
    <item>
      <title>EEG Foundation Models: Progresses, Benchmarking, and Open Problems</title>
      <link>https://huggingface.co/papers/2601.17883</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Huazhong University of Science and Technology | Authors: Dingkun Liu, Yuheng Chen, Zhu Chen, Zhenyao Cui, Yaozhi Wen</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.17883">arXiv</a> | <a href="https://arxiv.org/pdf/2601.17883.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	EEG foundation models show mixed performance compared to specialist models, with larger models not always providing better generalization under current data regimes.
				
					AI-generated summary</p>
  <p>摘要
	与专业模型相比，脑电图基础模型表现出好坏参半的性能，较大的模型并不总是在当前数据体系下提供更好的泛化能力。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.</p>
  <p>脑电图 (EEG) 基础模型最近已成为脑机接口 (BCI) 的一个有前途的范例，旨在从大规模异构记录中学习可转移的神经表征。尽管进展迅速，但由于预训练目标、预处理选择和下游评估协议不一致，现有脑电图基础模型缺乏公平和全面的比较。本文填补了这一空白。我们首先回顾了 50 个代表性模型，并将它们的设计选择组织成一个统一的分类框架，包括数据标准化、模型架构和自监督预训练策略。然后，我们评估了涵盖 9 个 BCI 范式的 13 个 EEG 数据集的 12 个开源基础模型和竞争性专家基线。强调现实世界的部署，我们考虑留一受试者协议下的跨受试者泛化和受试者内几次镜头设置下的快速校准。我们进一步将全参数微调与线性探测进行比较，以评估预训练表示的可转移性，并检查模型规模与下游性能之间的关系。我们的结果表明：1）线性探测常常是不够的； 2）从头开始训练的专业模型在许多任务中保持竞争力； 3）在当前的数据体系和训练实践下，更大的基础模型不一定会产生更好的泛化性能。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:07 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.17883</guid>
    </item>
    <item>
      <title>AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts</title>
      <link>https://huggingface.co/papers/2601.20730</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: OpenMOSS | Authors: Shicheng Fang, Yuxin Wang, XiaoRan Liu, Jiahao Lu, Chuanyuan Tan</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.20730">arXiv</a> | <a href="https://arxiv.org/pdf/2601.20730.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	AgentLongBench evaluates large language models as autonomous agents through dynamic environment interactions, revealing challenges in handling high-information-density tool responses compared to memory fragmentation in long conversations.
				
					AI-generated summary</p>
  <p>摘要
	AgentLongBench 通过动态环境交互将大型语言模型评估为自主代理，揭示了与长对话中的记忆碎片相比，处理高信息密度工具响应的挑战。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce AgentLongBench, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.</p>
  <p>大型语言模型 (LLM) 向自主代理的演变需要对广泛的动态上下文进行管理。然而，当前的基准在很大程度上仍然是静态的，依赖于无法模拟代理与环境交互的复杂性的被动检索任务，例如非线性推理和迭代反馈。为了解决这个问题，我们引入了 AgentLongBench，它通过基于横向思维难题的模拟环境推出来评估代理。该框架在知识密集型和无知识场景中生成严格的交互轨迹。对最先进的模型和内存系统（32K 到 4M 令牌）的实验暴露了一个严重的弱点：虽然擅长静态检索，但代理却难以处理工作流程所必需的动态信息合成。我们的分析表明，这种降级是由解决查询所需的最小令牌数量驱动的。这个因素解释了为什么大规模工具响应中固有的高信息密度比长轮对话中典型的记忆碎片带来了更大的挑战。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:07 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.20730</guid>
    </item>
    <item>
      <title>Exploring Reasoning Reward Model for Agents</title>
      <link>https://huggingface.co/papers/2601.22154</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Kaixuan Fan, Kaituo Feng, Manyuan Zhang, Tianshuo Peng, Zhixun Li</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.22154">arXiv</a> | <a href="https://arxiv.org/pdf/2601.22154.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Agent-RRM, a multi-faceted reward model, provides structured feedback for agentic trajectories through reasoning traces, critiques, and performance scores, with unified feedback integration showing superior performance across diverse benchmarks.
				
					AI-generated summary</p>
  <p>摘要
	Agent-RRM 是一种多方面的奖励模型，通过推理轨迹、批评和绩效评分为代理轨迹提供结构化反馈，并通过统一的反馈集成在不同的基准上显示出卓越的性能。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.</p>
  <p>代理强化学习（Agentic RL）在使代理能够执行复杂的推理和工具使用方面取得了显着的成功。然而，大多数方法仍然依赖于稀疏的基于结果的训练奖励。这种反馈无法区分中间推理质量，导致训练结果不理想。在本文中，我们介绍了智能体推理奖励模型（Agent-RRM），这是一种多方面的奖励模型，可为智能体轨迹产生结构化反馈，包括（1）明确的推理轨迹，（2）通过突出推理缺陷提供细化指导的集中批评，以及（3）评估过程性能的总体分数。利用这些信号，我们系统地研究了三种集成策略：Reagent-C（文本增强细化）、Reagent-R（奖励增强指导）和 Reagent-U（统一反馈集成）。对 12 个不同基准的广泛评估表明，Reagent-U 实现了显着的性能飞跃，在 GAIA 上实现了 43.7%，在 WebWalkerQA 上实现了 46.2%，验证了我们的推理奖励模型和训练方案的有效性。代码、模型和数据集均已发布，以方便未来的研究。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:07 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.22154</guid>
    </item>
    <item>
      <title>Qwen3-ASR Technical Report</title>
      <link>https://huggingface.co/papers/2601.21337</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Qwen | Authors: Xian Shi, Xiong Wang, Zhifang Guo, Yongqi Wang, Pei Zhang</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21337">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21337.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	The Qwen3-ASR family introduces speech recognition models with language identification capabilities and a non-autoregressive forced alignment model, achieving state-of-the-art performance and efficient processing.
				
					AI-generated summary</p>
  <p>摘要
	Qwen3-ASR系列引入了具有语言识别功能的语音识别模型和非自回归强制对齐模型，实现了最先进的性能和高效的处理。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.</p>
  <p>在本报告中，我们介绍了 Qwen3-ASR 系列，其中包括两个强大的一体式语音识别模型和一种新颖的非自回归语音强制对齐模型。 Qwen3-ASR-1.7B和Qwen3-ASR-0.6B是支持语言识别和52种语言和方言的ASR的ASR模型。两者都利用了大规模语音训练数据和基础模型 Qwen3-Omni 强大的音频理解能力。除了开源基准之外，我们还进行了全面的内部评估，因为 ASR 模型在开源基准分数上可能差异不大，但在现实场景中表现出显着的质量差异。实验表明，1.7B 版本在开源 ASR 模型中实现了 SOTA 性能，并且与最强的专有 API 具有竞争力，而 0.6B 版本则提供了最佳的准确性与效率权衡。 Qwen3-ASR-0.6B 可以实现低至 92ms 的平均 TTFT，并在 128 并发的情况下在 1 秒内转录 2000 秒语音。Qwen3-ForcedAligner-0.6B 是基于 LLM 的 NAR 时间戳预测器，能够对齐 11 种语言的文本语音对。时间戳精度实验表明，所提出的模型优于三种最强的力对齐模型，并且在效率和通用性方面更具优势。为了进一步加速 ASR 和音频理解的社区研究，我们在 Apache 2.0 许可证下发布了这些模型。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:07 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21337</guid>
    </item>
    <item>
      <title>PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction</title>
      <link>https://huggingface.co/papers/2601.22046</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: shanghai ailab | Authors: Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.22046">arXiv</a> | <a href="https://arxiv.org/pdf/2601.22046.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	PLANING presents an efficient streaming reconstruction framework that combines explicit geometric primitives with neural Gaussians to achieve high-quality rendering and accurate geometry simultaneously through decoupled optimization.
				
					AI-generated summary</p>
  <p>摘要
	PLANING 提出了一种高效的流式重建框架，将显式几何基元与神经高斯相结合，通过解耦优化同时实现高质量渲染和精确几何。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ .</p>
  <p>单目图像序列的流式重建仍然具有挑战性，因为现有方法通常倾向于高质量渲染或精确几何，但很少两者兼而有之。我们提出了 PLANING，这是一种基于混合表示的高效动态重建框架，它将显式几何基元与神经高斯松散耦合，从而能够以解耦的方式对几何和外观进行建模。这种解耦支持在线初始化和优化策略，将几何和外观更新分开，从而产生稳定的流式重建，并显着减少结构冗余。 PLANING 使密集网格 Chamfer-L2 比 PGSR 提高了 18.52%，PSNR 超过 ARTDECO 1.31 dB，并在 100 秒内重建 ScanNetV2 场景，比 2D Gaussian Splatting 快 5 倍以上，同时与离线每个场景优化的质量相匹配。除了重建质量之外，\modelname~的结构清晰度和计算效率使其非常适合广泛的下游应用，例如为具体人工智能实现大规模场景建模和模拟就绪环境。项目页面：https://city-super.github.io/PLANING/ 。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:07 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.22046</guid>
    </item>
    <item>
      <title>ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation</title>
      <link>https://huggingface.co/papers/2601.21420</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: ByteDance Seed | Authors: Zihao Huang, Jundong Zhou, Xingwei Qu, Qiyang Min, Ge Zhang</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21420">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21420.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	ConceptMoE dynamically allocates computation by merging similar tokens into concept representations, improving both performance and efficiency in large language models through adaptive processing and reduced attention computation.
				
					AI-generated summary</p>
  <p>摘要
	ConceptMoE 通过将相似的标记合并到概念表示中来动态分配计算，通过自适应处理和减少注意力计算来提高大型语言模型的性能和效率。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Large language models allocate uniform computation across all tokens, ignoring that some sequences are trivially predictable while others require deep reasoning. We introduce ConceptMoE, which dynamically merges semantically similar tokens into concept representations, performing implicit token-level compute allocation. A learnable chunk module identifies optimal boundaries by measuring inter-token similarity, compressing sequences by a target ratio R before they enter the compute-intensive concept model. Crucially, the MoE architecture enables controlled evaluation: we reallocate saved computation to match baseline activated FLOPs (excluding attention map computation) and total parameters, isolating genuine architectural benefits. Under these conditions, ConceptMoE consistently outperforms standard MoE across language and vision-language tasks, achieving +0.9 points on language pretraining, +2.3 points on long context understanding, and +0.6 points on multimodal benchmarks. When converting pretrained MoE during continual training with layer looping, gains reach +5.5 points, demonstrating practical applicability. Beyond performance, ConceptMoE reduces attention computation by up to R^2times and KV cache by Rtimes. At R=2, empirical measurements show prefill speedups reaching 175\% and decoding speedups up to 117\% on long sequences. The minimal architectural modifications enable straightforward integration into existing MoE, demonstrating that adaptive concept-level processing fundamentally improves both effectiveness and efficiency of large language models.</p>
  <p>大型语言模型在所有标记之间分配统一的计算，忽略了一些序列是可以简单预测的，而另一些序列则需要深度推理。我们引入了 ConceptMoE，它动态地将语义相似的标记合并到概念表示中，执行隐式标记级计算分配。可学习的块模块通过测量标记间相似性来识别最佳边界，在序列进入计算密集型概念模型之前按目标比率 R 压缩序列。至关重要的是，MoE 架构实现了受控评估：我们重新分配保存的计算以匹配基线激活的 FLOP（不包括注意力图计算）和总参数，从而隔离真正的架构优势。在这些条件下，ConceptMoE 在语言和视觉语言任务上始终优于标准 MoE，在语言预训练上获得 +0.9 分，在长上下文理解上获得 +2.3 分，在多模式基准上获得 +0.6 分。在通过层循环进行连续训练期间转换预训练的 MoE 时，增益达到 +5.5 点，证明了实际适用性。除了性能之外，ConceptMoE 将注意力计算减少了 R^2 倍，将 KV 缓存减少了 R 倍。在 R=2 时，经验测量显示长序列上的预填充加速达到 175\%，解码加速达到 117\%。最小的架构修改可以直接集成到现有的 MoE 中，这表明自适应概念级处理从根本上提高了大型语言模型的有效性和效率。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:07 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21420</guid>
    </item>
    <item>
      <title>OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models</title>
      <link>https://huggingface.co/papers/2601.21639</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Authors: Yufeng Zhong, Lei Chen, Xuanle Zhao, Wenkang Han, Liming Zheng</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21639">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21639.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	OCRVerse is a novel end-to-end OCR method that unifies text-centric and vision-centric approaches through comprehensive data engineering and a two-stage SFT-RL training framework with domain-specific reward strategies.
				
					AI-generated summary</p>
  <p>摘要
	OCRVerse 是一种新颖的端到端 OCR 方法，通过全面的数据工程和具有特定领域奖励策略的两阶段 SFT-RL 训练框架，统一了以文本为中心和以视觉为中心的方法。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.</p>
  <p>大视觉语言模型的发展推动了对海量多模态数据的管理和应用的需求，使得从视觉图像中提取信息的OCR技术越来越受欢迎。然而，现有的 OCR 方法主要侧重于从图像或扫描文档中识别文本元素（以文本为中心的 OCR），而忽略了从视觉信息密集的图像源中识别视觉元素（以视觉为中心的 OCR），例如图表、网页和科学绘图。事实上，这些视觉信息密集的图像在互联网上广泛存在，并且具有重要的现实应用价值，例如数据可视化和网页分析。在本技术报告中，我们提出了 OCRVerse，这是第一个端到端的整体 OCR 方法，可以实现统一的以文本为中心的 OCR 和以视觉为中心的 OCR。为此，我们构建了全面的数据工程，涵盖各种以文本为中心的文档，例如报纸、杂志和书籍，以及以视觉为中心的渲染复合材料，包括图表、网页和科学绘图。此外，我们还提出了一种针对 OCRVerse 的两阶段 SFT-RL 多域训练方法。 SFT直接混合跨领域数据来训练和建立初始领域知识，而RL则侧重于针对每个领域的特点设计个性化的奖励策略。具体来说，由于不同的领域需要不同的输出格式和预期输出，因此我们在 RL 阶段提供足够的灵活性，为每个领域定制灵活的奖励信号，从而提高跨领域融合并避免数据冲突。实验结果证明了 OCRVerse 的有效性，在以文本为中心和以视觉为中心的数据类型上取得了有竞争力的结果，甚至可以与大规模开源和闭源模型相媲美。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:07 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21639</guid>
    </item>
    <item>
      <title>MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods</title>
      <link>https://huggingface.co/papers/2601.21821</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: Shanghai Jiao Tong University | Authors: Honglin Lin, Zheng Liu, Yun Zhu, Chonghan Qin, Juekai Lin</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21821">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21821.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	A large-scale multimodal reasoning dataset called MMFineReason is introduced to improve vision language models' performance through high-quality reasoning annotations and demonstrates superior parameter efficiency in fine-tuned models.
				
					AI-generated summary</p>
  <p>摘要
	引入了名为 MMFineReason 的大规模多模态推理数据集，通过高质量推理注释来提高视觉语言模型的性能，并在微调模型中展示出卓越的参数效率。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a "less is more" phenomenon via our difficulty-aware filtering strategy: a subset of just 7\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.</p>
  <p>视觉语言模型 (VLM) 的最新进展推动了视觉推理领域的重大进展。然而，开源 VLM 仍然落后于专有系统，这很大程度上是由于缺乏高质量的推理数据。现有数据集对 STEM 图和视觉谜题等具有挑战性的领域的覆盖范围有限，并且缺乏对于引发强大推理能力至关重要的一致的长形式思想链 (CoT) 注释。为了弥补这一差距，我们引入了 MMFineReason，这是一个大规模多模态推理数据集，包含 180 万个样本和 5.1B 个解决方案标记，具有从 Qwen3-VL-235B-A22B-Thinking 中提取的高质量推理注释。该数据集通过系统的三阶段流程建立：（1）大规模数据收集和标准化，（2）CoT基本原理生成，（3）基于推理质量和难度意识的综合选择。生成的数据集涵盖 STEM 问题、视觉谜题、游戏和复杂图表，每个样本都用基于视觉的推理轨迹进行注释。我们在 MMFineReason 上微调 Qwen3-VL-Instruct 以开发 MMFineReason-2B/4B/8B 版本。我们的模型在其尺寸级别中建立了新的最先进的结果。值得注意的是，MMFineReason-4B 成功超越了 Qwen3-VL-8B-Thinking，MMFineReason-8B 甚至超越了 Qwen3-VL-30B-A3B-Thinking，同时接近 Qwen3-VL-32B-Thinking，展示了显着的参数效率。至关重要的是，我们通过难度感知过滤策略发现了“少即是多”的现象：仅 7%（123K 样本）的子集就实现了与完整数据集相当的性能。值得注意的是，我们揭示了一种协同效应，即面向推理的数据组合同时提高了一般能力。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:06 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21821</guid>
    </item>
    <item>
      <title>DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation</title>
      <link>https://huggingface.co/papers/2601.22153</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: MMLab@NTU | Authors: Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.22153">arXiv</a> | <a href="https://arxiv.org/pdf/2601.22153.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	DynamicVLA addresses dynamic object manipulation challenges through a compact vision-language-action model with temporal reasoning and closed-loop adaptation, supported by a new benchmark for dynamic manipulation tasks.
				
					AI-generated summary</p>
  <p>摘要
	DynamicVLA 通过具有时间推理和闭环适应的紧凑视觉-语言-动作模型解决动态对象操作挑战，并得到动态操作任务新基准的支持。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.</p>
  <p>操纵动态对象仍然是视觉-语言-动作（VLA）模型的一个开放挑战，尽管静态操纵具有很强的泛化性，但在需要快速感知、时间预测和连续控制的动态场景中却表现不佳。我们提出了 DynamicVLA，一个动态对象操作框架，通过三个关键设计集成了时间推理和闭环自适应：1）紧凑的 0.4B VLA，使用卷积视觉编码器进行空间高效、结构忠实的编码，从而实现快速多模态推理； 2）连续推理，实现重叠推理和执行，以降低延迟并及时适应对象运动； 3）潜在感知动作流，通过强制时间对齐的动作执行来弥补感知与执行之间的差距。为了填补动态操作数据的缺失基础，我们引入了动态对象操作 (DOM) 基准，该基准从头开始构建，具有自动数据收集管道，可有效收集 2.8K 场景和 206 个对象的 200K 合成片段，并无需远程操作即可快速收集 2K 真实世界片段。广泛的评估证明了响应速度、感知和泛化方面的显着改进，将 DynamicVLA 定位为跨实施例的通用动态对象操作的统一框架。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:06 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.22153</guid>
    </item>
    <item>
      <title>Scaling Embeddings Outperforms Scaling Experts in Language Models</title>
      <link>https://huggingface.co/papers/2601.21204</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: LongCat | Authors: Hong Liu, Jiaqi Zhang, Chao Wang, Xing Hu, Linkun Lyu</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.21204">arXiv</a> | <a href="https://arxiv.org/pdf/2601.21204.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Embedding scaling offers superior sparsity scaling compared to expert scaling in large language models, enabling efficient inference through system optimizations and speculative decoding.
				
					AI-generated summary</p>
  <p>摘要
	与大型语言模型中的专家缩放相比，嵌入缩放可提供卓越的稀疏性缩放，从而通过系统优化和推测解码实现高效推理。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.</p>
  <p>虽然专家混合 (MoE) 架构已成为大型语言模型中稀疏扩展的标准，但它们越来越面临收益递减和系统级瓶颈。在这项工作中，我们探索嵌入缩放作为缩放稀疏性的有效正交维度。通过全面的分析和实验，我们确定了与专家缩放相比，嵌入缩放实现了优越的帕累托前沿的特定机制。我们系统地描述了控制这种功效的关键架构因素——从参数预算到与模型宽度和深度的相互作用。此外，通过集成定制的系统优化和推测解码，我们有效地将这种稀疏性转化为有形的推理加速。在这些见解的指导下，我们推出了 LongCat-Flash-Lite，这是一个从头开始训练的 68.5B 参数模型，具有约 3B 激活值。尽管为嵌入分配了超过 30B 个参数，LongCat-Flash-Lite 不仅超越了参数等效的 MoE 基线，而且相对于同等规模的现有模型也表现出了卓越的竞争力，特别是在代理和编码领域。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:06 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.21204</guid>
    </item>
    <item>
      <title>Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models</title>
      <link>https://huggingface.co/papers/2601.20354</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: alibaba-inc | Authors: Zengbin Wang, Xuecai Hu, Yong Wang, Feng Xiong, Man Zhang</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.20354">arXiv</a> | <a href="https://arxiv.org/pdf/2601.20354.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.
				
					AI-generated summary</p>
  <p>摘要
	引入了新的基准和数据集，通过信息密集的提示和微调来评估和提高文本到图像模型的空间推理能力。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.</p>
  <p>文本到图像（T2I）模型在生成高保真图像方面取得了显着的成功，但它们常常无法处理复杂的空间关系，例如空间感知、推理或交互。由于提示设计简短或信息稀疏，当前的基准在很大程度上忽略了这些关键方面。在本文中，我们介绍了 SpatialGenEval，这是一个旨在系统评估 T2I 模型的空间智能的新基准，涵盖两个关键方面：（1）SpatialGenEval 涉及跨 25 个现实世界场景的 1,230 个长的、信息密集的提示。每个提示集成了10个空间子域和相应的10个多项选择问答对，范围从对象位置和布局到遮挡和因果关系。我们对 21 个最先进模型的广泛评估表明，高阶空间推理仍然是一个主要瓶颈。 (2) 为了证明我们的信息密集设计的实用性超出了简单的评估，我们还构建了 SpatialT2I 数据集。它包含 15,400 个文本-图像对，并带有重写的提示，以确保图像一致性，同时保留信息密度。对当前基础模型（即 Stable Diffusion-XL、Uniworld-V1、OmniGen2）的微调结果产生了一致的性能增益（+4.2%、+5.7%、+4.4%）和空间关系中更真实的效果，突出了以数据为中心的范例，以在 T2I 模型中实现空间智能。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:06 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.20354</guid>
    </item>
    <item>
      <title>Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives</title>
      <link>https://huggingface.co/papers/2601.20833</link>
      <description><![CDATA[<div class="paper-content">
  <p style="color: #666; font-size: 0.9em;">Institution: AgentAlpha | Authors: Tengyue Xu, Zhuoyang Qian, Gaoge Liu, Li Ling, Zhentao Zhang</p>
  <h3>arXiv Links</h3>
  <p><a href="https://arxiv.org/abs/2601.20833">arXiv</a> | <a href="https://arxiv.org/pdf/2601.20833.pdf">PDF</a></p>
  <h3>Abstract</h3>
  <p>Abstract
	Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.
				
					AI-generated summary</p>
  <p>摘要
	通过结构化方法图进行离线知识构建，减少对实时文献处理的依赖，从而实现更可靠和可扩展的自主科学发现。
				
					AI 生成的摘要</p>
  <h3>Detailed Abstract</h3>
  <p>Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.</p>
  <p>使用基于大语言模型（LLM）的代理的自主科学发现最近取得了实质性进展，展示了自动化端到端研究工作流程的能力。然而，现有系统很大程度上依赖于以运行时为中心的执行范例，对大量在线科学文献进行反复阅读、总结和推理。这种现场计算策略会产生很高的计算成本，受到上下文窗口的限制，并且经常导致脆弱的推理和幻觉。我们提出了 Idea2Story，这是一个用于自主科学发现的预计算驱动框架，它将文献理解从在线推理转变为离线知识构建。 Idea2Story不断收集同行评审的论文及其评审反馈，提取核心方法论单元，组成可重用的研究模式，并将它们组织成结构化的方法论知识图谱。在运行时，未明确的用户研究意图与已建立的研究范式保持一致，从而能够有效检索和重用高质量的研究模式，而不是开放式生成和试错。通过将研究计划和执行建立在预先构建的知识图中，Idea2Story 缓解了法学硕士的上下文窗口瓶颈，并大大减少了对文献的重复运行时推理。我们进行定性分析和初步实证研究，证明 Idea2Story 可以生成连贯的、有方法论依据的新颖的研究模式，并可以在端到端的环境中产生一些高质量的研究论证。这些结果表明，离线知识构建为可靠的自主科学发现提供了实用且可扩展的基础。</p>
</div>]]></description>
      <pubDate>Sat, 31 Jan 2026 12:55:06 GMT</pubDate>
      <guid isPermaLink="true">https://huggingface.co/papers/2601.20833</guid>
    </item>
  </channel>
</rss>
